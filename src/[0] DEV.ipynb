{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fed410c-b0ab-4a92-bc56-058377b2929f",
   "metadata": {},
   "source": [
    "# [0] DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3711dd35-6e77-47e0-aa49-6b801aa90100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Simulate only the first two CLI arguments\n",
    "sys.argv = [\n",
    "    'main_train.py',  # This would be the name of your script\n",
    "    '--hp', 'yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml',\n",
    "    '--devices', '0'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f27120-30ab-4007-b5fd-b41a7437c33c",
   "metadata": {},
   "source": [
    "#### TRAIN STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82f7133-0c9d-4e05-8bcf-5f42ca90de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import kappaprofiler as kp\n",
    "import yaml\n",
    "from torch.distributed import broadcast_object_list\n",
    "from wandb.util import generate_id\n",
    "\n",
    "from callbacks.base.callback_base import CallbackBase\n",
    "from configs.cli_args import CliArgs\n",
    "from configs.static_config import StaticConfig\n",
    "from configs.wandb_config import WandbConfig\n",
    "from datasets import dataset_from_kwargs\n",
    "from datasets.dummy_dataset import DummyDataset\n",
    "from distributed.config import is_rank0, is_distributed, get_rank, log_distributed_config\n",
    "from models import model_from_kwargs\n",
    "from models.dummy_model import DummyModel\n",
    "from providers.dataset_config_provider import DatasetConfigProvider\n",
    "from providers.path_provider import PathProvider\n",
    "from summarizers.stage_summarizers import stage_summarizer_from_kwargs\n",
    "from summarizers.summary_summarizers import summary_summarizer_from_kwargs\n",
    "from trainers import trainer_from_kwargs\n",
    "from utils.commands import command_from_kwargs\n",
    "from utils.data_container import DataContainer\n",
    "from utils.kappaconfig.util import save_unresolved_hp, save_resolved_hp, log_stage_hp\n",
    "from utils.logging_util import add_global_handlers\n",
    "from utils.memory_leak_util import get_tensors_in_memory\n",
    "from utils.seed import set_seed, get_random_int\n",
    "from utils.system_info import log_system_info, get_cli_command\n",
    "from utils.version_check import check_versions\n",
    "from utils.wandb_utils import init_wandb, finish_wandb\n",
    "\n",
    "\n",
    "def train_stage(stage_hp: dict, static_config: StaticConfig, cli_args: CliArgs, device: str):\n",
    "    # set environment variables\n",
    "    for key, value in stage_hp.get(\"env\", {}).items():\n",
    "        os.environ[key] = value if isinstance(value, str) else str(value)\n",
    "\n",
    "    # resume\n",
    "    if cli_args.resume_stage_id is not None:\n",
    "        assert \"initializer\" not in stage_hp[\"trainer\"]\n",
    "        if cli_args.resume_checkpoint is None:\n",
    "            checkpoint = \"latest\"\n",
    "        elif cli_args.resume_checkpoint.startswith(\"E\"):\n",
    "            checkpoint = dict(epoch=int(cli_args.resume_checkpoint[1:]))\n",
    "        elif cli_args.resume_checkpoint.startswith(\"U\"):\n",
    "            checkpoint = dict(update=int(cli_args.resume_checkpoint[1:]))\n",
    "        elif cli_args.resume_checkpoint.startswith(\"S\"):\n",
    "            checkpoint = dict(sample=int(cli_args.resume_checkpoint[1:]))\n",
    "        else:\n",
    "            # any checkpoint (like cp=last or cp=best.accuracy1.test.main)\n",
    "            checkpoint = cli_args.resume_checkpoint\n",
    "        stage_hp[\"trainer\"][\"initializer\"] = dict(\n",
    "            kind=\"resume_initializer\",\n",
    "            stage_id=cli_args.resume_stage_id,\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "\n",
    "    # retrieve stage_id from hp (allows queueing up dependent stages by hardcoding stage_ids in the yamls) e.g.:\n",
    "    # - pretrain MAE with stageid abcdefgh\n",
    "    # - finetune MAE where the backbone is initialized with the backbone from stage_id abcdefgh\n",
    "    stage_id = stage_hp.get(\"stage_id\", None)\n",
    "    # generate stage_id and sync across devices\n",
    "    if stage_id is None:\n",
    "        stage_id = generate_id()\n",
    "        if is_distributed():\n",
    "            object_list = [stage_id] if is_rank0() else [None]\n",
    "            broadcast_object_list(object_list)\n",
    "            stage_id = object_list[0]\n",
    "    stage_name = stage_hp.get(\"stage_name\", \"default_stage\")\n",
    "\n",
    "    # initialize logging\n",
    "    path_provider = PathProvider(\n",
    "        output_path=static_config.output_path,\n",
    "        model_path=static_config.model_path,\n",
    "        stage_name=stage_name,\n",
    "        stage_id=stage_id,\n",
    "        temp_path=static_config.temp_path,\n",
    "    )\n",
    "    message_counter = add_global_handlers(log_file_uri=path_provider.logfile_uri)\n",
    "\n",
    "    # init seed\n",
    "    run_name = cli_args.name or stage_hp.pop(\"name\", None)\n",
    "    seed = stage_hp.pop(\"seed\", None)\n",
    "    if seed is None:\n",
    "        seed = 0\n",
    "        logging.info(f\"no seed specified -> using seed={seed}\")\n",
    "\n",
    "    # initialize wandb\n",
    "    wandb_config_uri = stage_hp.pop(\"wandb\", None)\n",
    "    if wandb_config_uri == \"disabled\":\n",
    "        wandb_mode = \"disabled\"\n",
    "    else:\n",
    "        wandb_mode = cli_args.wandb_mode or static_config.default_wandb_mode\n",
    "    if wandb_mode == \"disabled\":\n",
    "        wandb_config_dict = {}\n",
    "        if cli_args.wandb_config is not None:\n",
    "            logging.warning(f\"wandb_config is defined via CLI but mode is disabled -> wandb_config is not used\")\n",
    "        if wandb_config_uri is not None:\n",
    "            logging.warning(f\"wandb_config is defined via yaml but mode is disabled -> wandb_config is not used\")\n",
    "    else:\n",
    "        # retrieve wandb config from yaml\n",
    "        if wandb_config_uri is not None:\n",
    "            wandb_config_uri = Path(\"wandb_configs\") / wandb_config_uri\n",
    "            if cli_args.wandb_config is not None:\n",
    "                logging.warning(f\"wandb_config is defined via CLI and via yaml -> wandb_config from yaml is used\")\n",
    "        # retrieve wandb config from --wandb_config cli arg\n",
    "        elif cli_args.wandb_config is not None:\n",
    "            wandb_config_uri = Path(\"wandb_configs\") / cli_args.wandb_config\n",
    "        # use default wandb_config file\n",
    "        else:\n",
    "            wandb_config_uri = Path(\"wandb_config.yaml\")\n",
    "        with open(wandb_config_uri.with_suffix(\".yaml\")) as f:\n",
    "            wandb_config_dict = yaml.safe_load(f)\n",
    "    wandb_config = WandbConfig(mode=wandb_mode, **wandb_config_dict)\n",
    "    config_provider, summary_provider = init_wandb(\n",
    "        device=device,\n",
    "        run_name=run_name,\n",
    "        stage_hp=stage_hp,\n",
    "        wandb_config=wandb_config,\n",
    "        path_provider=path_provider,\n",
    "        account_name=static_config.account_name,\n",
    "        tags=stage_hp.pop(\"tags\", None),\n",
    "        notes=stage_hp.pop(\"notes\", None),\n",
    "        group=stage_hp.pop(\"group\", None),\n",
    "        group_tags=stage_hp.pop(\"group_tags\", None),\n",
    "    )\n",
    "    # log codebase \"high-level\" version name (git commit is logged anyway)\n",
    "    config_provider[\"code/mlp\"] = \"CVSim\"\n",
    "    config_provider[\"code/tag\"] = os.popen(\"git describe --abbrev=0\").read().strip()\n",
    "    config_provider[\"code/name\"] = \"initial\"\n",
    "\n",
    "    # log setup\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(f\"stage_id: {stage_id}\")\n",
    "    logging.info(get_cli_command())\n",
    "    check_versions(verbose=True)\n",
    "    log_system_info()\n",
    "    static_config.log()\n",
    "    cli_args.log()\n",
    "    log_distributed_config()\n",
    "    log_stage_hp(stage_hp)\n",
    "    if is_rank0():\n",
    "        save_unresolved_hp(cli_args.hp, path_provider.stage_output_path / \"hp_unresolved.yaml\")\n",
    "        save_resolved_hp(stage_hp, path_provider.stage_output_path / \"hp_resolved.yaml\")\n",
    "\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(f\"training stage '{path_provider.stage_name}'\")\n",
    "    if is_distributed():\n",
    "        # using a different seed for every rank to ensure that stochastic processes are different across ranks\n",
    "        # for large batch_sizes this shouldn't matter too much\n",
    "        # this is relevant for:\n",
    "        # - augmentations (augmentation parameters of sample0 of rank0 == augparams of sample0 of rank1 == ...)\n",
    "        # - the masks of a MAE are the same for every rank\n",
    "        # NOTE: DDP syncs the parameters in its __init__ method -> same initial parameters independent of seed\n",
    "        seed += get_rank()\n",
    "        logging.info(f\"using different seeds per process (seed+rank) \")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # init datasets\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(\"initializing datasets\")\n",
    "    datasets = {}\n",
    "    dataset_config_provider = DatasetConfigProvider(\n",
    "        global_dataset_paths=static_config.get_global_dataset_paths(),\n",
    "        local_dataset_path=static_config.get_local_dataset_path(),\n",
    "        data_source_modes=static_config.get_data_source_modes(),\n",
    "    )\n",
    "    if \"datasets\" not in stage_hp:\n",
    "        logging.info(f\"no datasets found -> initialize dummy dataset\")\n",
    "        datasets[\"train\"] = DummyDataset(\n",
    "            size=256,\n",
    "            x_shape=(2,),\n",
    "            n_classes=2,\n",
    "        )\n",
    "    else:\n",
    "        for dataset_key, dataset_kwargs in stage_hp[\"datasets\"].items():\n",
    "            logging.info(f\"initializing {dataset_key}\")\n",
    "            datasets[dataset_key] = dataset_from_kwargs(\n",
    "                dataset_config_provider=dataset_config_provider,\n",
    "                path_provider=path_provider,\n",
    "                **dataset_kwargs,\n",
    "            )\n",
    "    data_container_kwargs = {}\n",
    "    if \"prefetch_factor\" in stage_hp:\n",
    "        data_container_kwargs[\"prefetch_factor\"] = stage_hp.pop(\"prefetch_factor\")\n",
    "    if \"max_num_workers\" in stage_hp:\n",
    "        data_container_kwargs[\"max_num_workers\"] = stage_hp.pop(\"max_num_workers\")\n",
    "    data_container = DataContainer(\n",
    "        **datasets,\n",
    "        num_workers=cli_args.num_workers,\n",
    "        pin_memory=cli_args.pin_memory,\n",
    "        config_provider=config_provider,\n",
    "        seed=get_random_int(),\n",
    "        **data_container_kwargs,\n",
    "    )\n",
    "\n",
    "    # init trainer\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(\"initializing trainer\")\n",
    "    trainer_kwargs = {}\n",
    "    if \"max_batch_size\" in stage_hp:\n",
    "        trainer_kwargs[\"max_batch_size\"] = stage_hp.pop(\"max_batch_size\")\n",
    "    trainer = trainer_from_kwargs(\n",
    "        data_container=data_container,\n",
    "        device=device,\n",
    "        sync_batchnorm=cli_args.sync_batchnorm or static_config.default_sync_batchnorm,\n",
    "        config_provider=config_provider,\n",
    "        summary_provider=summary_provider,\n",
    "        path_provider=path_provider,\n",
    "        **stage_hp[\"trainer\"],\n",
    "        **trainer_kwargs,\n",
    "    )\n",
    "    # register datasets of callbacks (e.g. for ImageNet-C the dataset never changes so its pointless to specify)\n",
    "    for callback in trainer.callbacks:\n",
    "        callback.register_root_datasets(\n",
    "            dataset_config_provider=dataset_config_provider,\n",
    "            is_mindatarun=cli_args.testrun or cli_args.mindatarun,\n",
    "        )\n",
    "\n",
    "    # init model\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(\"creating model\")\n",
    "    if \"model\" not in stage_hp:\n",
    "        logging.info(f\"no model defined -> use dummy model\")\n",
    "        model = DummyModel(\n",
    "            input_shape=trainer.input_shape,\n",
    "            output_shape=trainer.output_shape,\n",
    "            update_counter=trainer.update_counter,\n",
    "            path_provider=path_provider,\n",
    "            is_frozen=True,\n",
    "        )\n",
    "    else:\n",
    "        model = model_from_kwargs(\n",
    "            **stage_hp[\"model\"],\n",
    "            input_shape=trainer.input_shape,\n",
    "            output_shape=trainer.output_shape,\n",
    "            update_counter=trainer.update_counter,\n",
    "            path_provider=path_provider,\n",
    "            data_container=data_container,\n",
    "        )\n",
    "    \n",
    "    logging.info(f\"model architecture:\\n{model}\")\n",
    "    # moved to trainer as initialization on cuda is different than on cpu\n",
    "    # model = model.to(stage_config.run_config.device)\n",
    "\n",
    "    # train model\n",
    "    trainer.train_model(model)\n",
    "\n",
    "    # finish callbacks\n",
    "    CallbackBase.finish()\n",
    "\n",
    "    # summarize logvalues\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(f\"summarize logvalues\")\n",
    "    summary_provider.summarize_logvalues()\n",
    "\n",
    "    # summarize stage\n",
    "    if \"stage_summarizers\" in stage_hp and is_rank0():\n",
    "        logging.info(\"------------------\")\n",
    "        logging.info(\"summarize stage\")\n",
    "        for kwargs in stage_hp[\"stage_summarizers\"]:\n",
    "            summarizer = stage_summarizer_from_kwargs(\n",
    "                summary_provider=summary_provider,\n",
    "                path_provider=path_provider,\n",
    "                **kwargs,\n",
    "            )\n",
    "            summarizer.summarize()\n",
    "    # summarize summary\n",
    "    if \"summary_summarizers\" in stage_hp and is_rank0():\n",
    "        summary_provider.flush()\n",
    "        logging.info(\"------------------\")\n",
    "        for kwargs in stage_hp[\"summary_summarizers\"]:\n",
    "            summary_summarizer = summary_summarizer_from_kwargs(\n",
    "                summary_provider=summary_provider,\n",
    "                **kwargs,\n",
    "            )\n",
    "            summary_summarizer.summarize()\n",
    "    summary_provider.flush()\n",
    "\n",
    "    # add profiling times to summary_provider\n",
    "    def try_log_profiler_time(summary_key, profiler_query):\n",
    "        try:\n",
    "            summary_provider[summary_key] = kp.profiler.get_node(profiler_query).total_time\n",
    "        except AssertionError:\n",
    "            pass\n",
    "\n",
    "    try_log_profiler_time(\"profiler/train\", \"train\")\n",
    "    try_log_profiler_time(\"profiler/train/iterator\", \"train.iterator\")\n",
    "    try_log_profiler_time(\"profiler/train/data_loading\", \"train.data_loading\")\n",
    "    try_log_profiler_time(\"profiler/train/update\", \"train.update\")\n",
    "    try_log_profiler_time(\"profiler/train/to_device\", \"train.update.forward.to_device\")\n",
    "    try_log_profiler_time(\"profiler/train/forward\", \"train.update.forward\")\n",
    "    try_log_profiler_time(\"profiler/train/backward\", \"train.update.backward\")\n",
    "    summary_provider.flush()\n",
    "    # log profiler times\n",
    "    logging.info(f\"full profiling times:\\n{kp.profiler.to_string()}\")\n",
    "    kp.reset()\n",
    "\n",
    "    # execute commands\n",
    "    if \"on_finish\" in stage_hp and is_rank0():\n",
    "        logging.info(\"------------------\")\n",
    "        logging.info(\"ON_FINISH COMMANDS\")\n",
    "        for command in stage_hp[\"on_finish\"]:\n",
    "            command = command_from_kwargs(**command, stage_id=stage_id)\n",
    "            # noinspection PyBroadException\n",
    "            try:\n",
    "                command.execute()\n",
    "            except:\n",
    "                logging.exception(f\"failed to execute {command}\")\n",
    "\n",
    "    # cleanup\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(f\"CLEANUP\")\n",
    "    data_container.dispose()\n",
    "    message_counter.log()\n",
    "    finish_wandb(wandb_config)\n",
    "\n",
    "    # log how many tensors remain to be aware of potential memory leaks\n",
    "    all_tensors, cuda_tensors = get_tensors_in_memory()\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(f\"{len(all_tensors)} tensors remaining in memory (cpu+gpu)\")\n",
    "    logging.info(f\"{len(all_tensors) - len(cuda_tensors)} tensors remaining in memory (cpu)\")\n",
    "    logging.info(f\"{len(cuda_tensors)} tensors remaining in memory (gpu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20020b32-d493-455b-8d94-208d82711a8d",
   "metadata": {},
   "source": [
    "#### MAIN TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35d694a-142d-41ea-95ad-534dcdda8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executable: /home/ubuntu/UPT/upt_venv/bin/python\n",
      "executable: /home/ubuntu/UPT/upt_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "from utils.version_check import check_versions\n",
    "\n",
    "check_versions(verbose=False)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import kappaprofiler as kp\n",
    "import torch\n",
    "\n",
    "from configs.cli_args import parse_run_cli_args\n",
    "from configs.static_config import StaticConfig\n",
    "from distributed.config import barrier, get_rank, get_local_rank, get_world_size, is_managed\n",
    "from distributed.run import run_single_or_multiprocess, run_managed\n",
    "#from train_stage import train_stage\n",
    "from utils.kappaconfig.util import get_stage_hp\n",
    "from utils.logging_util import add_global_handlers, log_from_all_ranks\n",
    "from utils.pytorch_cuda_timing import cuda_start_event, cuda_end_event\n",
    "\n",
    "from main_train import main_single\n",
    "\n",
    "def main():\n",
    "    # parse cli_args immediately for fast cli_args validation\n",
    "    cli_args = parse_run_cli_args()\n",
    "    static_config = StaticConfig(uri=\"static_config.yaml\", datasets_were_preloaded=cli_args.datasets_were_preloaded)\n",
    "    # initialize loggers for setup (seperate id)\n",
    "    add_global_handlers(log_file_uri=None)\n",
    "\n",
    "    if is_managed():\n",
    "        run_managed(\n",
    "            accelerator=cli_args.accelerator,\n",
    "            devices=cli_args.devices,\n",
    "            main_single=main_single,\n",
    "        )\n",
    "    else:\n",
    "        run_single_or_multiprocess(\n",
    "            accelerator=cli_args.accelerator,\n",
    "            devices=cli_args.devices,\n",
    "            main_single=main_single,\n",
    "            master_port=cli_args.master_port or static_config.master_port,\n",
    "            mig_devices=static_config.mig_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04e4e0-e94f-4679-bd05-386c46a0e224",
   "metadata": {},
   "source": [
    "### Set Up CLI Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54ff0fc-a5dc-4c17-ab0a-3d2ded55f360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:49 I ------------------\n",
      "11-10 18:57:49 I running single process training\n",
      "11-10 18:57:49 I device 0: NVIDIA A10G (id=0)\n",
      "11-10 18:57:49 I initialized process rank=0 local_rank=0 pid=3082396\n",
      "11-10 18:57:49 I initialized 1 processes\n",
      "11-10 18:57:49 I initialized profiler to call sync cuda\n",
      "11-10 18:57:50 I log file: results/stage1/xsm5phdu/log.txt\n",
      "11-10 18:57:50 I no seed specified -> using seed=0\n",
      "11-10 18:57:50 I ------------------\n",
      "11-10 18:57:50 I initializing wandb (mode=online)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36577c52-702e-4c40-966c-0fe84de9e5fa",
   "metadata": {},
   "source": [
    "### Disect the Train Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace060bf-90bd-45fc-9423-8ef090771626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import yaml\n",
    "from torch.distributed import init_process_group, destroy_process_group, barrier\n",
    "from torch.multiprocessing import spawn\n",
    "\n",
    "#from .config import (\n",
    "#    is_managed,\n",
    "#    get_world_size_from_env,\n",
    "#    get_rank_from_env,\n",
    "#    get_local_rank,\n",
    "#    is_custom_managed_run,\n",
    "#    is_mpi_managed_run,\n",
    "#    get_nodes,\n",
    "#)\n",
    "\n",
    "def run_managed(accelerator, devices, main_single):\n",
    "    assert is_managed()\n",
    "    if accelerator == \"gpu\":\n",
    "        # custom managed run doesn't set CUDA_VISIBLE_DEVICES\n",
    "        if is_custom_managed_run() or is_mpi_managed_run() or len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")) > 1:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(get_local_rank())\n",
    "        _check_single_device_visible()\n",
    "    if devices is None:\n",
    "        world_size = get_world_size_from_env()\n",
    "        if world_size == 1:\n",
    "            _run_managed_singleprocess(accelerator, main_single)\n",
    "        else:\n",
    "            # use all GPUs for training\n",
    "            _run_managed_multiprocess(accelerator, main_single)\n",
    "    else:\n",
    "        # use single GPU (e.g. run_folder from every GPU)\n",
    "        world_size, device_ids = _parse_devices(accelerator, devices)\n",
    "        assert world_size == 1 and len(device_ids) == 1\n",
    "        _log_device_info(accelerator, device_ids)\n",
    "        _run_managed_singleprocess(accelerator, main_single)\n",
    "\n",
    "\n",
    "def _run_managed_singleprocess(accelerator, main_single):\n",
    "    # single process\n",
    "    logging.info(f\"running single process slurm training\")\n",
    "    device = _accelerator_to_device(accelerator)\n",
    "    main_single(device=device)\n",
    "\n",
    "\n",
    "def _run_managed_multiprocess(accelerator, main_single):\n",
    "    # setup MASTER_ADDR & MASTER_PORT\n",
    "    assert \"MASTER_ADDR\" in os.environ\n",
    "    assert \"MASTER_PORT\" in os.environ\n",
    "\n",
    "    # get config from env variables\n",
    "    world_size = get_world_size_from_env()\n",
    "    rank = get_rank_from_env()\n",
    "\n",
    "    # init process group\n",
    "    logging.info(\n",
    "        f\"initializing rank={rank} local_rank={get_local_rank()} \"\n",
    "        f\"nodes={get_nodes()} hostname={platform.uname().node} \"\n",
    "        f\"master_addr={os.environ['MASTER_ADDR']} master_port={os.environ['MASTER_PORT']} \"\n",
    "        f\"(waiting for all {world_size} processes to connect)\"\n",
    "    )\n",
    "    init_process_group(backend=get_backend(accelerator), init_method=\"env://\", world_size=world_size, rank=rank)\n",
    "    barrier()\n",
    "\n",
    "    # start main_single\n",
    "    device = _accelerator_to_device(accelerator)\n",
    "    main_single(device=device)\n",
    "    destroy_process_group()\n",
    "\n",
    "\n",
    "def run_single_or_multiprocess(accelerator, devices, main_single, master_port, mig_devices):\n",
    "    logging.info(\"------------------\")\n",
    "    # single node run\n",
    "    assert devices is not None\n",
    "    world_size, device_ids = _parse_devices(accelerator, devices, mig_devices)\n",
    "    if world_size == 1:\n",
    "        # single process\n",
    "        logging.info(f\"running single process training\")\n",
    "        if accelerator == \"gpu\":\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_ids[0]\n",
    "            _check_single_device_visible()\n",
    "        _log_device_info(accelerator, device_ids)\n",
    "        device = _accelerator_to_device(accelerator)\n",
    "        main_single(device=device)\n",
    "    else:\n",
    "        # spawn multi process training\n",
    "        logging.info(\n",
    "            f\"running multi process training on {world_size} processes \"\n",
    "            f\"(devices={devices} host={platform.uname().node})\"\n",
    "        )\n",
    "        master_port = _get_free_port(master_port)\n",
    "        logging.info(f\"master port: {master_port}\")\n",
    "        # dont log device info as this would load torch on device0 and block the VRAM required for this\n",
    "        # log_device_info(accelerator, device_ids)\n",
    "        args = (accelerator, device_ids, master_port, world_size, main_single)\n",
    "        spawn(_run_multiprocess, nprocs=world_size, args=args)\n",
    "\n",
    "\n",
    "def _run_multiprocess(rank, accelerator, device_ids, master_port, world_size, main_single):\n",
    "    # currently only single node is supported\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    \n",
    "    torch.cuda.set_device(int(device_ids[rank]))\n",
    "\n",
    "    if accelerator == \"gpu\":\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_ids[rank]\n",
    "        #_check_single_device_visible()\n",
    "\n",
    "    from torch.distributed import init_process_group, destroy_process_group\n",
    "    init_process_group(\n",
    "        backend=get_backend(accelerator, device_ids),\n",
    "        init_method=\"env://\",\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "    )\n",
    "    device = _accelerator_to_device(accelerator)\n",
    "    main_single(device=device)\n",
    "    destroy_process_group()\n",
    "\n",
    "\n",
    "def get_backend(accelerator, device_ids=None):\n",
    "    if accelerator == \"cpu\":\n",
    "        # gloo is recommended for cpu multiprocessing\n",
    "        # https://pytorch.org/docs/stable/distributed.html#which-backend-to-use\n",
    "        return \"gloo\"\n",
    "    if os.name == \"nt\":\n",
    "        # windows doesn't support nccl (I think)\n",
    "        return \"gloo\"\n",
    "    # MIG doesn't support NCCL\n",
    "    if device_ids is not None:\n",
    "        for device_id in device_ids:\n",
    "            try:\n",
    "                int(device_id)\n",
    "            except ValueError:\n",
    "                return \"gloo\"\n",
    "    # nccl is recommended for gpu multiprocessing\n",
    "    # https://pytorch.org/docs/stable/distributed.html#which-backend-to-use\n",
    "    return \"nccl\"\n",
    "\n",
    "\n",
    "def _get_free_port(start_port):\n",
    "    taken_ports = set()\n",
    "    for connection in psutil.net_connections():\n",
    "        if connection.laddr.ip == \"127.0.0.1\":\n",
    "            taken_ports.add(connection.laddr.port)\n",
    "        if len(connection.raddr) > 0 and connection.raddr.ip == \"127.0.0.1\":\n",
    "            taken_ports.add(connection.raddr.port)\n",
    "\n",
    "    for port in range(start_port, 65535):\n",
    "        if port not in taken_ports:\n",
    "            return port\n",
    "    raise ValueError(f\"all ports starting from {start_port} are taken\")\n",
    "\n",
    "\n",
    "def _parse_devices(accelerator, devices, mig_devices=None):\n",
    "    try:\n",
    "        # single process\n",
    "        device_ids = [int(devices)]\n",
    "    except ValueError:\n",
    "        # multi process\n",
    "        device_ids = yaml.safe_load(f\"[{devices}]\")\n",
    "        msg = f\"invalid devices specification '{devices}' (specify multiple devices like this '0,1,2,3')\"\n",
    "        assert all(isinstance(d, int) for d in device_ids), msg\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] requires string\n",
    "    device_ids = [str(device_id) for device_id in device_ids]\n",
    "\n",
    "    if accelerator == \"gpu\" and mig_devices is not None:\n",
    "        # map to MIG device ids\n",
    "        hostname = platform.uname().node\n",
    "        if hostname in mig_devices:\n",
    "            for i in range(len(device_ids)):\n",
    "                device_id = int(device_ids[i])\n",
    "                if device_id in mig_devices[hostname]:\n",
    "                    mig_device_id = mig_devices[hostname][device_id]\n",
    "                    device_ids[i] = mig_device_id\n",
    "                    logging.info(f\"device_id is MIG device with id {mig_device_id}\")\n",
    "\n",
    "    return len(device_ids), device_ids\n",
    "\n",
    "\n",
    "def _check_single_device_visible():\n",
    "    assert \"CUDA_VISIBLE_DEVICES\" in os.environ\n",
    "    visible_device_count = torch.cuda.device_count()\n",
    "    assert visible_device_count <= 1, os.environ\n",
    "\n",
    "\n",
    "def _log_device_info(accelerator, device_ids):\n",
    "    if accelerator == \"cpu\":\n",
    "        for i in range(len(device_ids)):\n",
    "            logging.info(f\"device {i}: cpu\")\n",
    "    elif accelerator == \"gpu\":\n",
    "        # retrieve device names via nvidia-smi because CUDA_VISIBLE_DEVICES needs to be set before calling anything\n",
    "        # in torch.cuda -> only 1 visible device\n",
    "        all_devices = os.popen(\"nvidia-smi --query-gpu=gpu_name --format=csv,noheader\").read().strip().split(\"\\n\")\n",
    "        for i, device_id in enumerate(device_ids):\n",
    "            try:\n",
    "                device_id = int(device_id)\n",
    "                logging.info(f\"device {i}: {all_devices[device_id]} (id={device_id})\")\n",
    "            except ValueError:\n",
    "                # MIG device\n",
    "                logging.info(f\"using MIG device\")\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def _accelerator_to_device(accelerator):\n",
    "    if accelerator == \"cpu\":\n",
    "        return \"cpu\"\n",
    "    elif accelerator == \"gpu\":\n",
    "        return \"cuda\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457df435-ef55-4ebb-b7f1-7501e208543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Simulate only the first two CLI arguments\n",
    "sys.argv = [\n",
    "    'main_train.py',  # This would be the name of your script\n",
    "    '--hp', 'yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml',\n",
    "    '--devices', '0'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c33091-2541-43a0-abac-bcb1ccac3302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MessageCounter (NOTSET)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse cli_args immediately for fast cli_args validation\n",
    "cli_args = parse_run_cli_args()\n",
    "static_config = StaticConfig(uri=\"static_config.yaml\", datasets_were_preloaded=cli_args.datasets_were_preloaded)\n",
    "# initialize loggers for setup (seperate id)\n",
    "add_global_handlers(log_file_uri=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1aabce-f42e-49b3-8d1d-8d45a34561cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator=cli_args.accelerator\n",
    "devices=cli_args.devices\n",
    "main_single=main_single\n",
    "master_port=cli_args.master_port or static_config.master_port\n",
    "mig_devices=static_config.mig_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b3280f-8349-412d-b7ea-ec279db1c773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:54 I running single process training\n",
      "11-10 18:57:54 I device 0: NVIDIA A10G (id=0)\n"
     ]
    }
   ],
   "source": [
    "# single process\n",
    "world_size, device_ids = _parse_devices(accelerator, devices, mig_devices)\n",
    "logging.info(f\"running single process training\")\n",
    "if accelerator == \"gpu\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_ids[0]\n",
    "    #_check_single_device_visible()\n",
    "_log_device_info(accelerator, device_ids)\n",
    "device = _accelerator_to_device(accelerator)\n",
    "#main_single(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28283bc6-b101-4c44-b48c-2b01127d12d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:55 I initialized process rank=0 local_rank=0 pid=3082396\n",
      "11-10 18:57:55 I initialized 1 processes\n",
      "11-10 18:57:55 I initialized profiler to call sync cuda\n"
     ]
    }
   ],
   "source": [
    "cli_args = parse_run_cli_args()\n",
    "static_config = StaticConfig(uri=\"static_config.yaml\", datasets_were_preloaded=cli_args.datasets_were_preloaded)\n",
    "add_global_handlers(log_file_uri=None)\n",
    "with log_from_all_ranks():\n",
    "    logging.info(f\"initialized process rank={get_rank()} local_rank={get_local_rank()} pid={os.getpid()}\")\n",
    "#barrier()\n",
    "logging.info(f\"initialized {get_world_size()} processes\")\n",
    "\n",
    "# CUDA_LAUNCH_BLOCKING=1 for debugging\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = str(1)\n",
    "\n",
    "# cudnn\n",
    "if cli_args.accelerator == \"gpu\":\n",
    "    if cli_args.cudnn_benchmark or static_config.default_cudnn_benchmark:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        assert not static_config.default_cudnn_deterministic, \"cudnn_benchmark can make things non-deterministic\"\n",
    "    else:\n",
    "        logging.warning(f\"disabled cudnn benchmark\")\n",
    "        if static_config.default_cudnn_deterministic:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "            logging.warning(f\"enabled cudnn deterministic\")\n",
    "\n",
    "# profiling\n",
    "if cli_args.accelerator == \"gpu\":\n",
    "    if cli_args.cuda_profiling or static_config.default_cuda_profiling:\n",
    "        kp.setup_async(cuda_start_event, cuda_end_event)\n",
    "        logging.info(f\"initialized profiler to call sync cuda\")\n",
    "else:\n",
    "    kp.setup_async_as_sync()\n",
    "\n",
    "# load hyperparameters\n",
    "stage_hp = get_stage_hp(\n",
    "    cli_args.hp,\n",
    "    template_path=\"zztemplates\",\n",
    "    testrun=cli_args.testrun,\n",
    "    minmodelrun=cli_args.minmodelrun,\n",
    "    mindatarun=cli_args.mindatarun,\n",
    "    mindurationrun=cli_args.mindurationrun,\n",
    ")\n",
    "\n",
    "# train stage\n",
    "#train_stage(\n",
    "#    stage_hp=stage_hp,\n",
    "#    static_config=static_config,\n",
    "#    cli_args=cli_args,\n",
    "#    device=device,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a2366-f06e-40a0-aba2-6f06a8d06bf2",
   "metadata": {},
   "source": [
    "### TRAIN STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b490d77-6835-401c-94c4-65b90e44e8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:57 I log file: results/stage1/ugse6jw9/log.txt\n",
      "11-10 18:57:57 I no seed specified -> using seed=0\n",
      "11-10 18:57:57 I ------------------\n",
      "11-10 18:57:57 I initializing wandb (mode=online)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablo-hermoso-moreno\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:57 I logged into wandb (host=https://api.wandb.ai/)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>results/stage1/ugse6jw9/wandb/run-20241110_185757-ugse6jw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pablo-hermoso-moreno/TEST-UPT/runs/ugse6jw9' target=\"_blank\">snc-all-sdfpos-e1000-subsam1-lr5e4-sdfperconly-seqlen1024-sdf512-cnext-dim768-sd02unif-reprcnn-grn-grid32/stage1</a></strong> to <a href='https://wandb.ai/pablo-hermoso-moreno/TEST-UPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pablo-hermoso-moreno/TEST-UPT' target=\"_blank\">https://wandb.ai/pablo-hermoso-moreno/TEST-UPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pablo-hermoso-moreno/TEST-UPT/runs/ugse6jw9' target=\"_blank\">https://wandb.ai/pablo-hermoso-moreno/TEST-UPT/runs/ugse6jw9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:57:58 I ------------------\n",
      "11-10 18:57:58 I stage_id: ugse6jw9\n",
      "11-10 18:57:58 I python main_train.py --hp yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml --devices 0\n",
      "11-10 18:57:58 I ------------------\n",
      "11-10 18:57:58 I VERSION CHECK\n",
      "11-10 18:57:58 I executable: /home/ubuntu/UPT/upt_venv/bin/python\n",
      "11-10 18:57:58 I python version: 3.10.9\n",
      "11-10 18:57:58 I torch version: 2.3.1+cu121\n",
      "11-10 18:57:58 I torch.cuda version: 12.1\n",
      "11-10 18:57:58 I torchvision.version: 0.18.1+cu121\n",
      "11-10 18:57:58 I kappabenchmark version: 0.0.10\n",
      "11-10 18:57:58 I kappaconfig version: 1.0.31\n",
      "11-10 18:57:58 I kappadata version: 1.4.15\n",
      "11-10 18:57:58 I kappamodules version: 0.1.99\n",
      "11-10 18:57:58 I kappaprofiler version: 1.0.11\n",
      "11-10 18:57:58 I kappaschedules version: 0.0.31\n",
      "11-10 18:57:58 I torchmetrics version: 1.4.3\n",
      "11-10 18:57:58 I ------------------\n",
      "11-10 18:57:58 I SYSTEM INFO\n",
      "11-10 18:57:58 I host name: ip-172-31-65-120\n",
      "11-10 18:57:58 I OS: Linux-5.15.0-1058-aws-x86_64-with-glibc2.31\n",
      "11-10 18:57:58 I OS version: #64~20.04.1-Ubuntu SMP Tue Apr 9 11:12:27 UTC 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:58:00 I CUDA version: 12.2\n",
      "11-10 18:58:00 W could not retrieve current git commit hash from ./.git/FETCH_HEAD\n",
      "11-10 18:58:00 I initialized process rank=0 local_rank=0 pid=3082396 hostname=ip-172-31-65-120\n",
      "11-10 18:58:00 I total_cpu_count: 192\n",
      "11-10 18:58:00 I ------------------\n",
      "11-10 18:58:00 I STATIC CONFIG\n",
      "11-10 18:58:00 I account_name: dev\n",
      "11-10 18:58:00 I output_path: results\n",
      "11-10 18:58:00 I local_dataset_path: /home/ubuntu/UPT/data\n",
      "11-10 18:58:00 I Filesystem      Size  Used Avail Use% Mounted on\n",
      "11-10 18:58:00 I /dev/root        16T  8.3T  6.9T  55% /\n",
      "11-10 18:58:00 I ------------------\n",
      "11-10 18:58:00 I CLI ARGS\n",
      "11-10 18:58:00 I hp: yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml\n",
      "11-10 18:58:00 I accelerator: gpu\n",
      "11-10 18:58:00 I devices: 0\n",
      "11-10 18:58:00 I testrun: False\n",
      "11-10 18:58:00 I minmodelrun: False\n",
      "11-10 18:58:00 I mindatarun: False\n",
      "11-10 18:58:00 I mindurationrun: False\n",
      "11-10 18:58:00 I datasets_were_preloaded: False\n",
      "11-10 18:58:00 I ------------------\n",
      "11-10 18:58:00 I DIST CONFIG\n",
      "11-10 18:58:00 I rank: 0\n",
      "11-10 18:58:00 I local_rank: 0\n",
      "11-10 18:58:00 I world_size: 1\n",
      "11-10 18:58:00 I nodes: 1\n",
      "11-10 18:58:00 I backend: None\n",
      "11-10 18:58:00 I ------------------\n",
      "stage_name: stage1\n",
      "datasets:\n",
      "  train:\n",
      "    kind: shapenet_car\n",
      "    split: train\n",
      "    grid_resolution: 32\n",
      "    concat_pos_to_sdf: true\n",
      "    collators:\n",
      "    - kind: rans_simformer_nognn_collator\n",
      "  test:\n",
      "    kind: shapenet_car\n",
      "    split: test\n",
      "    grid_resolution: 32\n",
      "    concat_pos_to_sdf: true\n",
      "    collators:\n",
      "    - kind: rans_simformer_nognn_collator\n",
      "model:\n",
      "  kind: rans_simformer_nognn_sdf_model\n",
      "  grid_encoder:\n",
      "    kind: encoders.rans_grid_convnext\n",
      "    patch_size: 2\n",
      "    kernel_size: 3\n",
      "    depthwise: false\n",
      "    global_response_norm: true\n",
      "    depths:\n",
      "    - 2\n",
      "    - 2\n",
      "    - 2\n",
      "    dims:\n",
      "    - 192\n",
      "    - 384\n",
      "    - 768\n",
      "    upsample_size: 64\n",
      "    upsample_mode: nearest\n",
      "    optim:\n",
      "      kind: adamw\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0.05\n",
      "      schedule:\n",
      "      - schedule:\n",
      "          kind: linear_increasing_schedule\n",
      "          exclude_first: true\n",
      "          exclude_last: true\n",
      "        end_epoch: 50\n",
      "      - schedule:\n",
      "          kind: cosine_decreasing_schedule\n",
      "          exclude_last: true\n",
      "          end_value: 1.0e-06\n",
      "  mesh_encoder:\n",
      "    kind: encoders.rans_perceiver\n",
      "    num_output_tokens: 1024\n",
      "    add_type_token: true\n",
      "    init_weights: truncnormal\n",
      "    kwargs:\n",
      "      dim: 768\n",
      "      num_attn_heads: 12\n",
      "    optim:\n",
      "      kind: adamw\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0.05\n",
      "      schedule:\n",
      "      - schedule:\n",
      "          kind: linear_increasing_schedule\n",
      "          exclude_first: true\n",
      "          exclude_last: true\n",
      "        end_epoch: 50\n",
      "      - schedule:\n",
      "          kind: cosine_decreasing_schedule\n",
      "          exclude_last: true\n",
      "          end_value: 1.0e-06\n",
      "  latent:\n",
      "    kind: latent.transformer_model\n",
      "    init_weights: truncnormal\n",
      "    drop_path_rate: 0.2\n",
      "    drop_path_decay: false\n",
      "    kwargs:\n",
      "      dim: 768\n",
      "      num_attn_heads: 12\n",
      "      depth: 12\n",
      "    optim:\n",
      "      kind: adamw\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0.05\n",
      "      schedule:\n",
      "      - schedule:\n",
      "          kind: linear_increasing_schedule\n",
      "          exclude_first: true\n",
      "          exclude_last: true\n",
      "        end_epoch: 50\n",
      "      - schedule:\n",
      "          kind: cosine_decreasing_schedule\n",
      "          exclude_last: true\n",
      "          end_value: 1.0e-06\n",
      "  decoder:\n",
      "    kind: decoders.rans_perceiver\n",
      "    init_weights: truncnormal\n",
      "    kwargs:\n",
      "      dim: 768\n",
      "      num_attn_heads: 12\n",
      "    optim:\n",
      "      kind: adamw\n",
      "      lr: 0.0005\n",
      "      weight_decay: 0.05\n",
      "      schedule:\n",
      "      - schedule:\n",
      "          kind: linear_increasing_schedule\n",
      "          exclude_first: true\n",
      "          exclude_last: true\n",
      "        end_epoch: 50\n",
      "      - schedule:\n",
      "          kind: cosine_decreasing_schedule\n",
      "          exclude_last: true\n",
      "          end_value: 1.0e-06\n",
      "trainer:\n",
      "  kind: rans_simformer_nognn_sdf_trainer\n",
      "  precision: bfloat16\n",
      "  backup_precision: float16\n",
      "  max_epochs: 100\n",
      "  effective_batch_size: 1\n",
      "  max_batch_size: 16\n",
      "  loss_function:\n",
      "    kind: elementwise_loss\n",
      "    loss_function:\n",
      "      kind: mse_loss\n",
      "  log_every_n_epochs: 1\n",
      "  callbacks:\n",
      "  - kind: offline_loss_callback\n",
      "    every_n_epochs: 1\n",
      "    dataset_key: test\n",
      "  - kind: best_checkpoint_callback\n",
      "    every_n_epochs: 1\n",
      "    metric_key: loss/test/total\n",
      "11-10 18:58:00 I copied unresolved hp to results/stage1/ugse6jw9/hp_unresolved.yaml\n",
      "11-10 18:58:00 I dumped resolved hp to results/stage1/ugse6jw9/hp_resolved.yaml\n",
      "11-10 18:58:00 I ------------------\n",
      "11-10 18:58:00 I training stage 'stage1'\n",
      "11-10 18:58:00 I set seed to 0\n"
     ]
    }
   ],
   "source": [
    "# set environment variables\n",
    "for key, value in stage_hp.get(\"env\", {}).items():\n",
    "    os.environ[key] = value if isinstance(value, str) else str(value)\n",
    "\n",
    "# resume\n",
    "if cli_args.resume_stage_id is not None:\n",
    "    assert \"initializer\" not in stage_hp[\"trainer\"]\n",
    "    if cli_args.resume_checkpoint is None:\n",
    "        checkpoint = \"latest\"\n",
    "    elif cli_args.resume_checkpoint.startswith(\"E\"):\n",
    "        checkpoint = dict(epoch=int(cli_args.resume_checkpoint[1:]))\n",
    "    elif cli_args.resume_checkpoint.startswith(\"U\"):\n",
    "        checkpoint = dict(update=int(cli_args.resume_checkpoint[1:]))\n",
    "    elif cli_args.resume_checkpoint.startswith(\"S\"):\n",
    "        checkpoint = dict(sample=int(cli_args.resume_checkpoint[1:]))\n",
    "    else:\n",
    "        # any checkpoint (like cp=last or cp=best.accuracy1.test.main)\n",
    "        checkpoint = cli_args.resume_checkpoint\n",
    "    stage_hp[\"trainer\"][\"initializer\"] = dict(\n",
    "        kind=\"resume_initializer\",\n",
    "        stage_id=cli_args.resume_stage_id,\n",
    "        checkpoint=checkpoint,\n",
    "    )\n",
    "\n",
    "# retrieve stage_id from hp (allows queueing up dependent stages by hardcoding stage_ids in the yamls) e.g.:\n",
    "# - pretrain MAE with stageid abcdefgh\n",
    "# - finetune MAE where the backbone is initialized with the backbone from stage_id abcdefgh\n",
    "stage_id = stage_hp.get(\"stage_id\", None)\n",
    "# generate stage_id and sync across devices\n",
    "if stage_id is None:\n",
    "    stage_id = generate_id()\n",
    "    if is_distributed():\n",
    "        object_list = [stage_id] if is_rank0() else [None]\n",
    "        broadcast_object_list(object_list)\n",
    "        stage_id = object_list[0]\n",
    "stage_name = stage_hp.get(\"stage_name\", \"default_stage\")\n",
    "\n",
    "# initialize logging\n",
    "path_provider = PathProvider(\n",
    "    output_path=static_config.output_path,\n",
    "    model_path=static_config.model_path,\n",
    "    stage_name=stage_name,\n",
    "    stage_id=stage_id,\n",
    "    temp_path=static_config.temp_path,\n",
    ")\n",
    "message_counter = add_global_handlers(log_file_uri=path_provider.logfile_uri)\n",
    "\n",
    "# init seed\n",
    "run_name = cli_args.name or stage_hp.pop(\"name\", None)\n",
    "seed = stage_hp.pop(\"seed\", None)\n",
    "if seed is None:\n",
    "    seed = 0\n",
    "    logging.info(f\"no seed specified -> using seed={seed}\")\n",
    "\n",
    "# initialize wandb\n",
    "wandb_config_uri = stage_hp.pop(\"wandb\", None)\n",
    "if wandb_config_uri == \"disabled\":\n",
    "    wandb_mode = \"disabled\"\n",
    "else:\n",
    "    wandb_mode = cli_args.wandb_mode or static_config.default_wandb_mode\n",
    "if wandb_mode == \"disabled\":\n",
    "    wandb_config_dict = {}\n",
    "    if cli_args.wandb_config is not None:\n",
    "        logging.warning(f\"wandb_config is defined via CLI but mode is disabled -> wandb_config is not used\")\n",
    "    if wandb_config_uri is not None:\n",
    "        logging.warning(f\"wandb_config is defined via yaml but mode is disabled -> wandb_config is not used\")\n",
    "else:\n",
    "    # retrieve wandb config from yaml\n",
    "    if wandb_config_uri is not None:\n",
    "        wandb_config_uri = Path(\"wandb_configs\") / wandb_config_uri\n",
    "        if cli_args.wandb_config is not None:\n",
    "            logging.warning(f\"wandb_config is defined via CLI and via yaml -> wandb_config from yaml is used\")\n",
    "    # retrieve wandb config from --wandb_config cli arg\n",
    "    elif cli_args.wandb_config is not None:\n",
    "        wandb_config_uri = Path(\"wandb_configs\") / cli_args.wandb_config\n",
    "    # use default wandb_config file\n",
    "    else:\n",
    "        wandb_config_uri = Path(\"wandb_config.yaml\")\n",
    "    with open(wandb_config_uri.with_suffix(\".yaml\")) as f:\n",
    "        wandb_config_dict = yaml.safe_load(f)\n",
    "wandb_config = WandbConfig(mode=wandb_mode, **wandb_config_dict)\n",
    "config_provider, summary_provider = init_wandb(\n",
    "    device=device,\n",
    "    run_name=run_name,\n",
    "    stage_hp=stage_hp,\n",
    "    wandb_config=wandb_config,\n",
    "    path_provider=path_provider,\n",
    "    account_name=static_config.account_name,\n",
    "    tags=stage_hp.pop(\"tags\", None),\n",
    "    notes=stage_hp.pop(\"notes\", None),\n",
    "    group=stage_hp.pop(\"group\", None),\n",
    "    group_tags=stage_hp.pop(\"group_tags\", None),\n",
    ")\n",
    "# log codebase \"high-level\" version name (git commit is logged anyway)\n",
    "config_provider[\"code/mlp\"] = \"CVSim\"\n",
    "config_provider[\"code/tag\"] = os.popen(\"git describe --abbrev=0\").read().strip()\n",
    "config_provider[\"code/name\"] = \"initial\"\n",
    "\n",
    "# log setup\n",
    "logging.info(\"------------------\")\n",
    "logging.info(f\"stage_id: {stage_id}\")\n",
    "logging.info(get_cli_command())\n",
    "check_versions(verbose=True)\n",
    "log_system_info()\n",
    "static_config.log()\n",
    "cli_args.log()\n",
    "log_distributed_config()\n",
    "log_stage_hp(stage_hp)\n",
    "if is_rank0():\n",
    "    save_unresolved_hp(cli_args.hp, path_provider.stage_output_path / \"hp_unresolved.yaml\")\n",
    "    save_resolved_hp(stage_hp, path_provider.stage_output_path / \"hp_resolved.yaml\")\n",
    "\n",
    "logging.info(\"------------------\")\n",
    "logging.info(f\"training stage '{path_provider.stage_name}'\")\n",
    "if is_distributed():\n",
    "    # using a different seed for every rank to ensure that stochastic processes are different across ranks\n",
    "    # for large batch_sizes this shouldn't matter too much\n",
    "    # this is relevant for:\n",
    "    # - augmentations (augmentation parameters of sample0 of rank0 == augparams of sample0 of rank1 == ...)\n",
    "    # - the masks of a MAE are the same for every rank\n",
    "    # NOTE: DDP syncs the parameters in its __init__ method -> same initial parameters independent of seed\n",
    "    seed += get_rank()\n",
    "    logging.info(f\"using different seeds per process (seed+rank) \")\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "448fe8b7-509b-4306-80d6-803aef0f1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:58:00 I ------------------\n",
      "11-10 18:58:00 I initializing datasets\n",
      "11-10 18:58:00 I initializing train\n",
      "11-10 18:58:01 I data_source (global): '/home/ubuntu/UPT/data/shapenet_car_processed'\n",
      "11-10 18:58:01 I data_source (local): '/home/ubuntu/UPT/data/shapenet_car'\n",
      "11-10 18:58:01 I initializing test\n",
      "11-10 18:58:01 I data_source (global): '/home/ubuntu/UPT/data/shapenet_car_processed'\n",
      "11-10 18:58:01 I data_source (local): '/home/ubuntu/UPT/data/shapenet_car'\n"
     ]
    }
   ],
   "source": [
    "# init datasets\n",
    "logging.info(\"------------------\")\n",
    "logging.info(\"initializing datasets\")\n",
    "datasets = {}\n",
    "dataset_config_provider = DatasetConfigProvider(\n",
    "    global_dataset_paths=static_config.get_global_dataset_paths(),\n",
    "    local_dataset_path=static_config.get_local_dataset_path(),\n",
    "    data_source_modes=static_config.get_data_source_modes(),\n",
    ")\n",
    "if \"datasets\" not in stage_hp:\n",
    "    logging.info(f\"no datasets found -> initialize dummy dataset\")\n",
    "    datasets[\"train\"] = DummyDataset(\n",
    "        size=256,\n",
    "        x_shape=(2,),\n",
    "        n_classes=2,\n",
    "    )\n",
    "else:\n",
    "    for dataset_key, dataset_kwargs in stage_hp[\"datasets\"].items():\n",
    "        logging.info(f\"initializing {dataset_key}\")\n",
    "        datasets[dataset_key] = dataset_from_kwargs(\n",
    "            dataset_config_provider=dataset_config_provider,\n",
    "            path_provider=path_provider,\n",
    "            **dataset_kwargs,\n",
    "        )\n",
    "data_container_kwargs = {}\n",
    "if \"prefetch_factor\" in stage_hp:\n",
    "    data_container_kwargs[\"prefetch_factor\"] = stage_hp.pop(\"prefetch_factor\")\n",
    "if \"max_num_workers\" in stage_hp:\n",
    "    data_container_kwargs[\"max_num_workers\"] = stage_hp.pop(\"max_num_workers\")\n",
    "data_container = DataContainer(\n",
    "    **datasets,\n",
    "    num_workers=cli_args.num_workers,\n",
    "    pin_memory=cli_args.pin_memory,\n",
    "    config_provider=config_provider,\n",
    "    seed=get_random_int(),\n",
    "    **data_container_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a5f4f1-1725-4842-abc5-c77d09bf4d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.data_container.DataContainer at 0x7fe731718700>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e662807-606d-4e39-a726-11b199a4e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:58:01 I ------------------\n",
      "11-10 18:58:01 I initializing trainer\n",
      "11-10 18:58:01 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)\n",
      "11-10 18:58:01 I main_sampler: RandomSampler(num_repeats=1)\n"
     ]
    }
   ],
   "source": [
    "# init trainer\n",
    "logging.info(\"------------------\")\n",
    "logging.info(\"initializing trainer\")\n",
    "trainer_kwargs = {}\n",
    "if \"max_batch_size\" in stage_hp:\n",
    "    trainer_kwargs[\"max_batch_size\"] = stage_hp.pop(\"max_batch_size\")\n",
    "trainer = trainer_from_kwargs(\n",
    "    data_container=data_container,\n",
    "    device=device,\n",
    "    sync_batchnorm=cli_args.sync_batchnorm or static_config.default_sync_batchnorm,\n",
    "    config_provider=config_provider,\n",
    "    summary_provider=summary_provider,\n",
    "    path_provider=path_provider,\n",
    "    **stage_hp[\"trainer\"],\n",
    "    **trainer_kwargs,\n",
    ")\n",
    "# register datasets of callbacks (e.g. for ImageNet-C the dataset never changes so its pointless to specify)\n",
    "for callback in trainer.callbacks:\n",
    "    callback.register_root_datasets(\n",
    "        dataset_config_provider=dataset_config_provider,\n",
    "        is_mindatarun=cli_args.testrun or cli_args.mindatarun,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c57be1b-a6a9-47b7-a40f-6086ad1ed432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.data_container.DataContainer at 0x7fe731718700>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "809d653e-3594-42bb-a707-5defa6101a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9492fd13-8286-4859-bb71-a1c73928df3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:47:12 I ------------------\n",
      "11-10 18:47:12 I creating model\n",
      "11-10 18:47:12 I model has no initializers -> not loading a checkpoint or an optimizer state\n",
      "11-10 18:47:12 I model has no initializers -> not loading a checkpoint or an optimizer state\n",
      "11-10 18:47:13 I model has no initializers -> not loading a checkpoint or an optimizer state\n",
      "11-10 18:47:13 I model has no initializers -> not loading a checkpoint or an optimizer state\n",
      "11-10 18:47:14 I model has no initializers -> not loading a checkpoint or an optimizer state\n",
      "11-10 18:47:14 I model architecture:\n",
      "RansSimformerNognnSdfModel(\n",
      "  (grid_encoder): RansGridConvnext(\n",
      "    (model): ConvNext(\n",
      "      (stem): Sequential(\n",
      "        (0): Conv3d(4, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (1): LayerNorm3d(\n",
      "          (layer): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (stages): ModuleList(\n",
      "        (0): ConvNextStage(\n",
      "          (downsampling): Identity()\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(192, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(768, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "            (1): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(192, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(768, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ConvNextStage(\n",
      "          (downsampling): Sequential(\n",
      "            (0): LayerNorm3d(\n",
      "              (layer): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Conv3d(192, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(384, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(1536, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "            (1): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(384, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(1536, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ConvNextStage(\n",
      "          (downsampling): Sequential(\n",
      "            (0): LayerNorm3d(\n",
      "              (layer): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Conv3d(384, 768, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(768, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(3072, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "            (1): ConvNextBlock(\n",
      "              (drop_path): DropPath(drop_prob=0.000)\n",
      "              (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "              (norm): LayerNorm3d(\n",
      "                (layer): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Conv3d(768, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "                (act): GELU(approximate='none')\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Conv3d(3072, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mesh_encoder): RansPerceiver(\n",
      "    (pos_embed): ContinuousSincosEmbed(dim=768)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "    (block): PerceiverPoolingBlock(\n",
      "      (perceiver): PerceiverBlock(\n",
      "        (norm1q): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (norm1kv): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): PerceiverAttention1d(\n",
      "          (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
      "          (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (drop_path1): DropPath(drop_prob=0.000)\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (drop_path2): DropPath(drop_prob=0.000)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (latent): TransformerModel(\n",
      "    (input_proj): LinearProjection(\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x PrenormBlock(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): DotProductAttention1d(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): DropPath(drop_prob=0.200)\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): DropPath(drop_prob=0.200)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): RansPerceiver(\n",
      "    (proj): LinearProjection(\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (pos_embed): ContinuousSincosEmbed(dim=768)\n",
      "    (query_mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (perceiver): PerceiverBlock(\n",
      "      (norm1q): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm1kv): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): PerceiverAttention1d(\n",
      "        (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.000)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.000)\n",
      "    )\n",
      "    (norm): Identity()\n",
      "    (pred): LinearProjection(\n",
      "      (proj): Linear(in_features=768, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "logging.info(\"------------------\")\n",
    "logging.info(\"creating model\")\n",
    "if \"model\" not in stage_hp:\n",
    "    logging.info(f\"no model defined -> use dummy model\")\n",
    "    model = DummyModel(\n",
    "        input_shape=trainer.input_shape,\n",
    "        output_shape=trainer.output_shape,\n",
    "        update_counter=trainer.update_counter,\n",
    "        path_provider=path_provider,\n",
    "        is_frozen=True,\n",
    "    )\n",
    "else:\n",
    "    model = model_from_kwargs(\n",
    "        **stage_hp[\"model\"],\n",
    "        input_shape=trainer.input_shape,\n",
    "        output_shape=trainer.output_shape,\n",
    "        update_counter=trainer.update_counter,\n",
    "        path_provider=path_provider,\n",
    "        data_container=data_container,\n",
    "    )\n",
    "\n",
    "logging.info(f\"model architecture:\\n{model}\")\n",
    "# moved to trainer as initialization on cuda is different than on cpu\n",
    "# model = model.to(stage_config.run_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06b88c95-1974-46d2-a25d-c91c61b7b8d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-10 18:47:26 I rans_grid_convnext applying model specific initialization\n",
      "11-10 18:47:26 I rans_perceiver applying model specific initialization\n",
      "11-10 18:47:26 I transformer_model applying model specific initialization\n",
      "11-10 18:47:26 I rans_perceiver applying model specific initialization\n",
      "11-10 18:47:26 I applying model specific initialization\n",
      "11-10 18:47:26 I rans_grid_convnext initialize optimizer\n",
      "11-10 18:47:26 I base lr: 5e-4\n",
      "11-10 18:47:26 I scaled lr: 1.95e-6\n",
      "11-10 18:47:26 I lr_scaler=LinearLrScaler(divisor=256)\n",
      "11-10 18:47:26 I lr_scale_factor=1\n",
      "11-10 18:47:26 I group modifiers exclude_bias_from_wd=True exclude_norm_from_wd=True add_model_specific_param_group_modifiers=True [ExcludeFromWdByNameModifier(name=type_token)]\n",
      "11-10 18:47:26 I using 2 param groups:\n",
      "11-10 18:47:26 I weight_decay=0.0 len(params)=52\n",
      "11-10 18:47:26 I len(params)=21\n",
      "11-10 18:47:26 I rans_perceiver initialize optimizer\n",
      "11-10 18:47:26 I base lr: 5e-4\n",
      "11-10 18:47:26 I scaled lr: 1.95e-6\n",
      "11-10 18:47:26 I lr_scaler=LinearLrScaler(divisor=256)\n",
      "11-10 18:47:26 I lr_scale_factor=1\n",
      "11-10 18:47:26 I group modifiers exclude_bias_from_wd=True exclude_norm_from_wd=True add_model_specific_param_group_modifiers=True [ExcludeFromWdByNameModifier(name=block.query) ExcludeFromWdByNameModifier(name=type_token)]\n",
      "11-10 18:47:26 I using 2 param groups:\n",
      "11-10 18:47:26 I weight_decay=0.0 len(params)=15\n",
      "11-10 18:47:26 I len(params)=7\n",
      "11-10 18:47:26 I transformer_model initialize optimizer\n",
      "11-10 18:47:26 I base lr: 5e-4\n",
      "11-10 18:47:26 I scaled lr: 1.95e-6\n",
      "11-10 18:47:26 I lr_scaler=LinearLrScaler(divisor=256)\n",
      "11-10 18:47:26 I lr_scale_factor=1\n",
      "11-10 18:47:26 I group modifiers exclude_bias_from_wd=True exclude_norm_from_wd=True add_model_specific_param_group_modifiers=True []\n",
      "11-10 18:47:26 I using 2 param groups:\n",
      "11-10 18:47:26 I len(params)=49\n",
      "11-10 18:47:26 I weight_decay=0.0 len(params)=97\n",
      "11-10 18:47:26 I rans_perceiver initialize optimizer\n",
      "11-10 18:47:26 I base lr: 5e-4\n",
      "11-10 18:47:26 I scaled lr: 1.95e-6\n",
      "11-10 18:47:26 I lr_scaler=LinearLrScaler(divisor=256)\n",
      "11-10 18:47:26 I lr_scale_factor=1\n",
      "11-10 18:47:26 I group modifiers exclude_bias_from_wd=True exclude_norm_from_wd=True add_model_specific_param_group_modifiers=True []\n",
      "11-10 18:47:26 I using 2 param groups:\n",
      "11-10 18:47:26 I len(params)=9\n",
      "11-10 18:47:26 I weight_decay=0.0 len(params)=15\n",
      "11-10 18:47:26 I added default EtaCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I added default DatasetStatsCallback\n",
      "11-10 18:47:26 I added default ParamCountCallback\n",
      "11-10 18:47:26 I added default CopyPreviousConfigCallback\n",
      "11-10 18:47:26 I added default CopyPreviousSummaryCallback\n",
      "11-10 18:47:26 I added default ProgressCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I added default TrainTimeCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I added default OnlineLossCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I added default LrCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I added default FreezerCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I added default OnlineLossCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I torch.compile not used (use_torch_compile == False)\n",
      "11-10 18:47:26 I ------------------\n",
      "11-10 18:47:26 I PREPARE TRAINER\n",
      "11-10 18:47:26 I calculating batch_size and accumulation_steps (effective_batch_size=1)\n",
      "11-10 18:47:26 I effective_batch_size: 1\n",
      "11-10 18:47:26 I using provided max_batch_size 16 (16 per device)\n",
      "11-10 18:47:26 I batch_size: 1\n",
      "11-10 18:47:26 I accumulation_steps: 1\n",
      "11-10 18:47:26 I train_batches per epoch: 700 (world_size=1 batch_size=1)\n",
      "11-10 18:47:26 I initializing dataloader\n",
      "11-10 18:47:26 I OfflineLossCallback(every_n_epochs=1) registered InterleavedSamplerConfig(every_n_epochs=1) dataset_mode='pressure mesh_pos sdf query_pos'\n",
      "11-10 18:47:26 I created dataloader (batch_size=1 num_workers=24 pin_memory=True total_cpu_count=192 prefetch_factor=2)\n",
      "11-10 18:47:26 I concatenated dataset properties:\n",
      "11-10 18:47:26 I - mode='pressure mesh_pos sdf query_pos' len=700 root_dataset=ShapenetCar\n",
      "11-10 18:47:26 I - mode='pressure mesh_pos sdf query_pos' len=189 root_dataset=ShapenetCar\n",
      "11-10 18:47:26 I ------------------\n",
      "11-10 18:47:26 I BEFORE TRAINING\n",
      "11-10 18:47:26 I train: 700 samples\n",
      "11-10 18:47:26 I test: 189 samples\n",
      "11-10 18:47:26 I parameter counts (trainable | frozen)\n",
      "11-10 18:47:26 I  164,298,049 | 0 | total\n",
      "11-10 18:47:26 I   57,192,000 | 0 | grid_encoder.rans_grid_convnext\n",
      "11-10 18:47:26 I   12,599,040 | 0 | mesh_encoder.rans_perceiver\n",
      "11-10 18:47:26 I   85,645,056 | 0 | latent.transformer_model\n",
      "11-10 18:47:26 I    8,861,953 | 0 | decoder.rans_perceiver\n",
      "11-10 18:47:26 I ------------------\n",
      "11-10 18:47:26 I EtaCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I DatasetStatsCallback\n",
      "11-10 18:47:26 I ParamCountCallback\n",
      "11-10 18:47:26 I CopyPreviousConfigCallback\n",
      "11-10 18:47:26 I CopyPreviousSummaryCallback\n",
      "11-10 18:47:26 I ProgressCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I TrainTimeCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I OnlineLossCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I LrCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I FreezerCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I OnlineLossCallback(every_n_updates=50)\n",
      "11-10 18:47:26 I OfflineLossCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I BestCheckpointCallback(every_n_epochs=1)\n",
      "11-10 18:47:26 I ------------------\n",
      "11-10 18:47:26 I START TRAINING\n",
      "11-10 18:47:26 I initializing dataloader workers\n",
      "11-10 18:47:28 I initialized dataloader workers\n",
      "11-10 18:47:28 I 0 unused parameters\n",
      "E   0/100 U     1/70000 S    1.0/70.0K | next_log   1/700 | next_log_eta 18:49:39 (02:11->00:00) | training_eta 12-01:04:11 (1-06:16:43.00->00:00:01.55) | avg_update 0.19s\r"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/kappaprofiler/__init__.py:20\u001b[0m, in \u001b[0;36mprofile.<locals>._profile\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_profile\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profiler\u001b[38;5;241m.\u001b[39mprofile(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/src/trainers/base/sgd_trainer.py:521\u001b[0m, in \u001b[0;36mSgdTrainer.train_model\u001b[0;34m(self, model, callbacks)\u001b[0m\n\u001b[1;32m    519\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_loader(periodic_callbacks\u001b[38;5;241m=\u001b[39mperiodic_callbacks, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_before_training(trainer_model\u001b[38;5;241m=\u001b[39mtrainer_model, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddp_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddp_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batches_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperiodic_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperiodic_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_after_training(trainer_model\u001b[38;5;241m=\u001b[39mtrainer_model, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/UPT/src/trainers/base/sgd_trainer.py:674\u001b[0m, in \u001b[0;36mSgdTrainer._train\u001b[0;34m(self, model, trainer_model, ddp_model, batch_size, accumulation_steps, data_loader, train_batches_per_epoch, periodic_callbacks)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# no end of epoch -> flush logs from call_after_update\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last_update_in_epoch:\n\u001b[0;32m--> 674\u001b[0m     \u001b[43mCallbackBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# check update/sample based early stopping\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/UPT/src/callbacks/base/callback_base.py:36\u001b[0m, in \u001b[0;36mCallbackBase.flush\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflush\u001b[39m():\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CallbackBase\u001b[38;5;241m.\u001b[39mlog_writer_singleton \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mCallbackBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_writer_singleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/src/callbacks/base/writers/log_writer.py:69\u001b[0m, in \u001b[0;36mLogWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# wandb doesn't support querying offline logfiles so offline mode would have no way to summarize stages\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# also fetching the summaries from the online version potentially takes a long time, occupying GPU servers\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# for primitive tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# -------------------\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# check that every log is fully cached (i.e. no update is logged twice)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_entries[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# don't keep histograms for primitive logging\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_entries\u001b[38;5;241m.\u001b[39mappend({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_cache\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, wandb\u001b[38;5;241m.\u001b[39mHistogram)})\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8263ad9630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f8263ad9630>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "terminate called after throwing an instance of 'c10::Error'\n",
      "  what():  CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f83b8f13897 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f83b8ec3b25 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f83b8feb718 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x7f83b8feb9f2 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\n",
      "frame #4: <unknown function> + 0x104588a (0x7f836181988a in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: <unknown function> + 0x5a5950 (0x7f83b961a950 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x6a36f (0x7f83b8ef836f in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #7: c10::TensorImpl::~TensorImpl() + 0x21b (0x7f83b8ef11cb in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #8: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f83b8ef1379 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #9: <unknown function> + 0x851088 (0x7f83b98c6088 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: THPVariable_subclass_dealloc(_object*) + 0x2f6 (0x7f83b98c6406 in /home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0x12c917 (0x55650d2b0917 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #12: <unknown function> + 0x1201ab (0x55650d2a41ab in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #13: <unknown function> + 0x139028 (0x55650d2bd028 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #14: <unknown function> + 0x148f7f (0x55650d2ccf7f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #15: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #16: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #17: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #18: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #19: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #20: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #21: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #22: <unknown function> + 0x148fdd (0x55650d2ccfdd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #23: <unknown function> + 0x14e344 (0x55650d2d2344 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #24: <unknown function> + 0x12c986 (0x55650d2b0986 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #25: <unknown function> + 0x19f6c1 (0x55650d3236c1 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #26: <unknown function> + 0x123025 (0x55650d2a7025 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #27: <unknown function> + 0x1d4d6e (0x55650d358d6e in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #28: <unknown function> + 0x13f506 (0x55650d2c3506 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #29: _PyEval_EvalFrameDefault + 0x2857 (0x55650d2b6647 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #30: <unknown function> + 0x1860c7 (0x55650d30a0c7 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #31: <unknown function> + 0x19fd64 (0x55650d323d64 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #32: _PyEval_EvalFrameDefault + 0x1215 (0x55650d2b5005 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #33: _PyFunction_Vectorcall + 0x6f (0x55650d2c3f8f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #34: <unknown function> + 0xb4ce (0x7f83dea084ce in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #35: <unknown function> + 0xab55 (0x7f83dea07b55 in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #36: <unknown function> + 0x97be (0x7f83dea067be in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #37: <unknown function> + 0xb747 (0x7f83dea08747 in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #38: <unknown function> + 0x9a2a (0x7f83dea06a2a in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #39: <unknown function> + 0x97be (0x7f83dea067be in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #40: <unknown function> + 0xb747 (0x7f83dea08747 in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #41: <unknown function> + 0x9a2a (0x7f83dea06a2a in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #42: <unknown function> + 0xa681 (0x7f83dea07681 in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #43: <unknown function> + 0x97be (0x7f83dea067be in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #44: <unknown function> + 0x97be (0x7f83dea067be in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #45: <unknown function> + 0x13ee0 (0x7f83dea10ee0 in /opt/conda/lib/python3.10/lib-dynload/_pickle.cpython-310-x86_64-linux-gnu.so)\n",
      "frame #46: <unknown function> + 0x14a63f (0x55650d2ce63f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #47: _PyEval_EvalFrameDefault + 0x735 (0x55650d2b4525 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #48: <unknown function> + 0x14b641 (0x55650d2cf641 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #49: _PyEval_EvalFrameDefault + 0x4d0d (0x55650d2b8afd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #50: _PyFunction_Vectorcall + 0x6f (0x55650d2c3f8f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #51: _PyEval_EvalFrameDefault + 0x2ec2 (0x55650d2b6cb2 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #52: _PyFunction_Vectorcall + 0x6f (0x55650d2c3f8f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #53: _PyEval_EvalFrameDefault + 0x332 (0x55650d2b4122 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #54: _PyFunction_Vectorcall + 0x6f (0x55650d2c3f8f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #55: _PyEval_EvalFrameDefault + 0x735 (0x55650d2b4525 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #56: _PyFunction_Vectorcall + 0x6f (0x55650d2c3f8f in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #57: _PyEval_EvalFrameDefault + 0x735 (0x55650d2b4525 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #58: <unknown function> + 0x14b8cd (0x55650d2cf8cd in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #59: <unknown function> + 0x22c1e5 (0x55650d3b01e5 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #60: <unknown function> + 0x22c194 (0x55650d3b0194 in /home/ubuntu/UPT/upt_venv/bin/python)\n",
      "frame #61: <unknown function> + 0x8609 (0x7f83df958609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #62: clone + 0x43 (0x7f83df717353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1868, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.10/selectors.py\", line 469, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/home/ubuntu/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 3082185) is killed by signal: Aborted. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m:  View run \u001b[33msnc-all-sdfpos-e1000-subsam1-lr5e4-sdfperconly-seqlen1024-sdf512-cnext-dim768-sd02unif-reprcnn-grn-grid32/stage1\u001b[0m at: \u001b[34mhttps://wandb.ai/pablo-hermoso-moreno/TEST-UPT/runs/x1alubub\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5773717-244e-48f8-95fd-70615b3126a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish callbacks\n",
    "CallbackBase.finish()\n",
    "\n",
    "# summarize logvalues\n",
    "logging.info(\"------------------\")\n",
    "logging.info(f\"summarize logvalues\")\n",
    "summary_provider.summarize_logvalues()\n",
    "\n",
    "# summarize stage\n",
    "if \"stage_summarizers\" in stage_hp and is_rank0():\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(\"summarize stage\")\n",
    "    for kwargs in stage_hp[\"stage_summarizers\"]:\n",
    "        summarizer = stage_summarizer_from_kwargs(\n",
    "            summary_provider=summary_provider,\n",
    "            path_provider=path_provider,\n",
    "            **kwargs,\n",
    "        )\n",
    "        summarizer.summarize()\n",
    "# summarize summary\n",
    "if \"summary_summarizers\" in stage_hp and is_rank0():\n",
    "    summary_provider.flush()\n",
    "    logging.info(\"------------------\")\n",
    "    for kwargs in stage_hp[\"summary_summarizers\"]:\n",
    "        summary_summarizer = summary_summarizer_from_kwargs(\n",
    "            summary_provider=summary_provider,\n",
    "            **kwargs,\n",
    "        )\n",
    "        summary_summarizer.summarize()\n",
    "summary_provider.flush()\n",
    "\n",
    "# add profiling times to summary_provider\n",
    "def try_log_profiler_time(summary_key, profiler_query):\n",
    "    try:\n",
    "        summary_provider[summary_key] = kp.profiler.get_node(profiler_query).total_time\n",
    "    except AssertionError:\n",
    "        pass\n",
    "\n",
    "try_log_profiler_time(\"profiler/train\", \"train\")\n",
    "try_log_profiler_time(\"profiler/train/iterator\", \"train.iterator\")\n",
    "try_log_profiler_time(\"profiler/train/data_loading\", \"train.data_loading\")\n",
    "try_log_profiler_time(\"profiler/train/update\", \"train.update\")\n",
    "try_log_profiler_time(\"profiler/train/to_device\", \"train.update.forward.to_device\")\n",
    "try_log_profiler_time(\"profiler/train/forward\", \"train.update.forward\")\n",
    "try_log_profiler_time(\"profiler/train/backward\", \"train.update.backward\")\n",
    "summary_provider.flush()\n",
    "# log profiler times\n",
    "logging.info(f\"full profiling times:\\n{kp.profiler.to_string()}\")\n",
    "kp.reset()\n",
    "\n",
    "# execute commands\n",
    "if \"on_finish\" in stage_hp and is_rank0():\n",
    "    logging.info(\"------------------\")\n",
    "    logging.info(\"ON_FINISH COMMANDS\")\n",
    "    for command in stage_hp[\"on_finish\"]:\n",
    "        command = command_from_kwargs(**command, stage_id=stage_id)\n",
    "        # noinspection PyBroadException\n",
    "        try:\n",
    "            command.execute()\n",
    "        except:\n",
    "            logging.exception(f\"failed to execute {command}\")\n",
    "\n",
    "# cleanup\n",
    "logging.info(\"------------------\")\n",
    "logging.info(f\"CLEANUP\")\n",
    "data_container.dispose()\n",
    "message_counter.log()\n",
    "finish_wandb(wandb_config)\n",
    "\n",
    "# log how many tensors remain to be aware of potential memory leaks\n",
    "all_tensors, cuda_tensors = get_tensors_in_memory()\n",
    "logging.info(\"------------------\")\n",
    "logging.info(f\"{len(all_tensors)} tensors remaining in memory (cpu+gpu)\")\n",
    "logging.info(f\"{len(all_tensors) - len(cuda_tensors)} tensors remaining in memory (cpu)\")\n",
    "logging.info(f\"{len(cuda_tensors)} tensors remaining in memory (gpu)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74645b29-3697-4166-8d38-67a9a6391a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upt_venv",
   "language": "python",
   "name": "upt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
