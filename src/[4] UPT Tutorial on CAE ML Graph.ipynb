{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f78c976-050a-4d8e-a48d-8f7774eb3db9",
   "metadata": {},
   "source": [
    "# 4 UPT Tutorial on CAE ML Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fca86c-71a0-43d3-a73f-d3dacf9ab42b",
   "metadata": {},
   "source": [
    "This graph is from the SS-INR2-SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb5c469-18f9-4a98-86b4-000b6c5d0d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: Graph(num_nodes=87345, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'pos': Scheme(shape=(3,), dtype=torch.float32), 'sv': Scheme(shape=(3,), dtype=torch.float32), 'y': Scheme(shape=(2,), dtype=torch.float32), 'node_type': Scheme(shape=(1,), dtype=torch.float32)}\n",
      "      edata_schemes={'edge_type': Scheme(shape=(1,), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int32), 'x': Scheme(shape=(8,), dtype=torch.float32)})\n",
      "Number of nodes: 87345\n",
      "Number of edges: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# Load the DGL graph from a binary file\n",
    "graph_list, _ = dgl.load_graphs(\"test_graph.bin\")\n",
    "\n",
    "# If there is only one graph in the file, retrieve it\n",
    "graph = graph_list[0]\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Number of nodes:\", graph.num_nodes())\n",
    "print(\"Number of edges:\", graph.num_edges())\n",
    "\n",
    "# If the graph contains node or edge features, access them as follows\n",
    "if 'feat' in graph.ndata:\n",
    "    print(\"Node features:\", graph.ndata['feat'])\n",
    "if 'feat' in graph.edata:\n",
    "    print(\"Edge features:\", graph.edata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801cc9d6-5634-489e-ba4e-c922fc6b71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.ndata['x'] = torch.concat([graph.ndata['pos'], graph.ndata['sv']],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ca3609-a697-4827-bf6f-2ce439ec0ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87345, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.ndata['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453d604-31fa-406b-9af0-2bf521742439",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder processes input features (e.g. velocities, pressure, ...) and input positions at timestep \n",
    " and encodes it into a latent representation \n",
    ". Input features and input positions are sparse tensors. The input features are first processed with a shallow MLP. Then, the input positions are added to the result of the MLP. This representation is then used for a message passing, where messages are only passed to selected supernodes. We randomly select a fixed number nodes from each pointcloud during dataloading which are then used as \"supernodes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe96693-c772-404b-919e-cc9b5637c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.encoders.rans_perceiver import RansPerceiver as EncoderRansPerceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5935170-b6e1-4b8b-b429-64b2ea71b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_encoder = EncoderRansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    num_output_tokens = 1024,\n",
    "    add_type_token = True,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = (None, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfe2494-5112-45ed-95b6-49af61ccce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = torch.zeros(graph.ndata[\"x\"].shape[0], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3034fafb-62bf-4354-8edf-9d276cbc6926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87345])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5dc89b-1809-4dc4-bf7b-06318e4b22ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d70b88e-c88e-4ba8-b2f2-54262be5e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_embed = mesh_encoder(mesh_pos=graph.ndata[\"x\"], batch_idx=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f04adaf2-2739-44ba-9fba-280b77b30261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c16f814-6d4d-4b06-ae12-aed35c418bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5807,  0.4658,  0.1716,  ..., -0.7107, -0.5791, -0.0913],\n",
       "         [ 0.5517, -1.2879,  0.6687,  ..., -1.1653, -0.3798, -0.5626],\n",
       "         [-0.1468, -0.9169,  1.4105,  ...,  1.0836, -0.6124,  0.4049],\n",
       "         ...,\n",
       "         [-0.9655,  0.8354,  1.0881,  ...,  0.3756, -1.3532, -1.2547],\n",
       "         [-1.3816, -0.4939,  1.3127,  ...,  0.5757,  0.6239, -0.0216],\n",
       "         [ 1.8845,  1.2737, -0.2964,  ...,  1.0076, -0.6336,  0.4241]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe7421-95a5-4f4b-a6e6-63dc92f9b65b",
   "metadata": {},
   "source": [
    "#### Approximator\n",
    "\n",
    "The approximator takes the latent_tokens and pushes them forward by one timestep. It simply consists of some transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac380704-3d02-4911-ab52-4e6198268c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.latent.transformer_model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883b2dc3-f801-4cdc-81e9-843b07e09b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = TransformerModel(\n",
    "    init_weights = \"truncnormal\",\n",
    "    drop_path_rate = 0.2,\n",
    "    drop_path_decay = False,\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    depth = 12,\n",
    "    input_shape = mesh_encoder.output_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "570aed60-207d-4d39-b99a-fbb01458d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "propagated = latent(mesh_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75574d3c-11c9-4b0e-a18e-1cb48726830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "245c0ebd-44c0-4ace-bd02-0cba14e7a63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4416, -0.4059,  0.1636,  ...,  0.4360,  0.7620,  2.4100],\n",
       "         [-0.0073, -1.2096, -0.7920,  ...,  1.8539, -1.7077,  1.3626],\n",
       "         [ 0.2752,  2.5950, -0.6274,  ...,  1.3275,  1.4254,  2.3314],\n",
       "         ...,\n",
       "         [ 0.7461,  0.2784,  0.9726,  ..., -0.7214,  0.8626,  0.2719],\n",
       "         [-2.0826,  1.3264,  1.0127,  ...,  1.7330, -0.0511,  1.8760],\n",
       "         [-0.2851,  0.2323,  0.1737,  ...,  1.5888,  0.7301,  2.9876]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2cedfc-f8a8-47c0-bbfd-e0df9f061daa",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The decoder takes the latent_tokens and decodes them into the original space of the input data. It does this by querying the latent space at arbitrary positions. It first employs some transformer blocks, followed by a perceiver decoder block. The output positions are encoded via a shallow MLP before being used as query vector for the perceiver.\n",
    "\n",
    "For training, the output positions need to have an associated ground truth value in the dataset as the model is trained via an mean-squared-error loss between the predictions at the output positions and the ground truth value at the output positions. For inference, the output positions can be arbitrary. Also: output positions and input positions do not have to match (also not during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0cfd83-0995-4143-8b6d-f2de29b4d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.decoders.rans_perceiver import RansPerceiver as DecoderRansPerceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcbc25da-34b3-4d3f-9444-3c673aab3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = latent.output_shape,\n",
    "    output_shape = (None, 1),\n",
    "    static_ctx = {\"ndim\":6} # Not Sure.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59ea521c-db7f-4f5d-bb08-a78d442cb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated, query_pos=graph.ndata[\"x\"].unsqueeze(dim=0), unbatch_idx=batch_idx, unbatch_select=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c546ed-b710-453c-b4d7-93d47a224c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87345, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc8fe84-bc86-4ed0-92ea-4bf3d639c4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0411],\n",
       "        [ 0.0393],\n",
       "        [ 0.0218],\n",
       "        ...,\n",
       "        [-0.0245],\n",
       "        [ 0.0553],\n",
       "        [ 0.0005]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b68005f-8218-4f7a-8937-00e81f6ce32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87345, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.ndata[\"y\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc624044-8dce-46b5-800f-3674bb79ae4c",
   "metadata": {},
   "source": [
    "### Add support for Image Output: Taking Inspitation from the Decoder Perceiver from UPT Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd486a0d-59c1-4c06-9ac6-f57983ebcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "from kappamodules.layers import ContinuousSincosEmbed, LinearProjection, Sequential\n",
    "from kappamodules.transformer import PerceiverBlock, Mlp, DitPerceiverBlock, DitBlock\n",
    "from kappamodules.vit import VitBlock\n",
    "from torch_geometric.utils import unbatch\n",
    "from torch import nn\n",
    "from models.base.single_model_base import SingleModelBase\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "class RansPerceiver(SingleModelBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_attn_heads,\n",
    "        init_weights=\"xavier_uniform\",\n",
    "        init_last_proj_zero=False,\n",
    "        use_last_norm=False,\n",
    "        output_mode=\"sparse\",  \n",
    "        num_images=1,  # Number of \"image-slots\" in the channel dimension\n",
    "        image_dims=None,  # (height, width) for image output\n",
    "        last_activation=None,  # <-- New parameter\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `last_activation` can be:\n",
    "            - None                (no activation, unbounded)\n",
    "            - \"sigmoid\"          (clamps output to [0,1])\n",
    "            - \"tanh_shift_scale\" (maps output to [0,1] via 0.5 * (tanh(x) + 1))\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.num_images = num_images  \n",
    "        self.use_last_norm = use_last_norm\n",
    "        self.output_mode = output_mode\n",
    "        self.image_dims = image_dims\n",
    "        self.last_activation = last_activation  # <-- Store last_activation\n",
    "\n",
    "        # Input projection\n",
    "        _, input_dim = self.input_shape\n",
    "        self.proj = LinearProjection(input_dim, dim, init_weights=init_weights)\n",
    "\n",
    "        # Query tokens (positional embedding + MLP)\n",
    "        self.pos_embed = ContinuousSincosEmbed(dim=dim, ndim=self.static_ctx[\"ndim\"])\n",
    "        self.query_mlp = Mlp(in_dim=dim, hidden_dim=dim, init_weights=init_weights)\n",
    "\n",
    "        # Transformer block (latent to tokens)\n",
    "        self.perceiver = PerceiverBlock(\n",
    "            dim=dim,\n",
    "            num_heads=num_attn_heads,\n",
    "            init_last_proj_zero=init_last_proj_zero,\n",
    "            init_weights=init_weights,\n",
    "        )\n",
    "\n",
    "        # Figure out final channels (e.g., if output_dim=3 (RGB) then final is 3 * num_images)\n",
    "        _, output_dim = self.output_shape\n",
    "        final_channels = output_dim * num_images\n",
    "\n",
    "        # Final projection\n",
    "        self.pred = LinearProjection(dim, final_channels, init_weights=init_weights)\n",
    "\n",
    "        # Optional normalization\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6) if use_last_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x, query_pos, unbatch_idx, unbatch_select):\n",
    "        \"\"\"\n",
    "        x          : [batch_size, latent_seq_len, dim]\n",
    "        query_pos  : [batch_size, height*width, pos_dim]\n",
    "        \"\"\"\n",
    "        # 1) Project input\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # 2) Create query embeddings\n",
    "        query_pos_embed = self.pos_embed(query_pos)      # [batch_size, height*width, dim]\n",
    "        query = self.query_mlp(query_pos_embed)          # [batch_size, height*width, dim]\n",
    "\n",
    "        # 3) Perceiver decoding\n",
    "        x = self.perceiver(q=query, kv=x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pred(x)\n",
    "        # Now shape: [batch_size, height*width, output_dim*num_images]\n",
    "\n",
    "        # 4) Reshape / rearrange based on self.output_mode\n",
    "        if self.output_mode == \"sparse\":\n",
    "            # E.g. for point-cloud style outputs\n",
    "            x = einops.rearrange(\n",
    "                x, \n",
    "                \"batch_size max_num_points (channels) -> (batch_size max_num_points) channels\", \n",
    "                channels=self.output_shape[1]  # e.g. 3\n",
    "            )\n",
    "            unbatched = unbatch(x, batch=unbatch_idx)\n",
    "            x = torch.concat([unbatched[i] for i in unbatch_select])\n",
    "\n",
    "        elif self.output_mode == \"dense_to_sparse_unpadded\":\n",
    "            x = einops.rearrange(x, \"b seqlen c -> (b seqlen) c\")\n",
    "\n",
    "        elif self.output_mode == \"image\":\n",
    "            # Reshape to [batch_size, num_images, output_dim, height, width]\n",
    "            if self.image_dims is not None:\n",
    "                height, width = self.image_dims\n",
    "\n",
    "                # We expect exactly height*width tokens\n",
    "                expected_tokens = height * width\n",
    "                assert x.size(1) == expected_tokens, (\n",
    "                    f\"Expected {expected_tokens} tokens (height*width), got {x.size(1)}\"\n",
    "                )\n",
    "\n",
    "                batch_size = x.size(0)\n",
    "                output_dim = self.output_shape[1]  # e.g., 3 (RGB)\n",
    "                \n",
    "                # Step-by-step reshape:\n",
    "                # Currently => [b, height*width, num_images*output_dim]\n",
    "                # 1) [b, hw, num_images, output_dim]\n",
    "                x = x.view(batch_size, height * width, self.num_images, output_dim)\n",
    "\n",
    "                # 2) [b, num_images, output_dim, hw]\n",
    "                x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "                # 3) [b, num_images, output_dim, height, width]\n",
    "                x = x.view(batch_size, self.num_images, output_dim, height, width)\n",
    "\n",
    "                # Optional final dimension reorder if you prefer [b, num_images, width, height, output_dim]:\n",
    "                x = x.permute(0, 1, 4, 3, 2)\n",
    "            else:\n",
    "                raise ValueError(\"image_dims must be provided for 'image' output mode.\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output mode: {self.output_mode}\")\n",
    "\n",
    "        # 5) Apply optional final activation\n",
    "        if self.last_activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        elif self.last_activation == \"tanh_shift_scale\":\n",
    "            # Map from [-1, +1] to [0, 1]\n",
    "            x = 0.5 * (torch.tanh(x) + 1.0)\n",
    "        elif self.last_activation is not None:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported value for last_activation='{self.last_activation}'. \"\n",
    "                \"Use None, 'sigmoid', or 'tanh_shift_scale'.\"\n",
    "            )\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderPerceiver(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            ndim,\n",
    "            dim,\n",
    "            depth,\n",
    "            num_heads,\n",
    "            unbatch_mode=\"dense_to_sparse_unpadded\",\n",
    "            perc_dim=None,\n",
    "            perc_num_heads=None,\n",
    "            cond_dim=None,\n",
    "            init_weights=\"truncnormal002\",\n",
    "            num_images=1, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        perc_dim = perc_dim or dim\n",
    "        perc_num_heads = perc_num_heads or num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.ndim = ndim\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.perc_dim = perc_dim\n",
    "        self.perc_num_heads = perc_num_heads\n",
    "        self.cond_dim = cond_dim\n",
    "        self.init_weights = init_weights\n",
    "        self.unbatch_mode = unbatch_mode\n",
    "        self.num_images = num_images\n",
    "\n",
    "        # input projection\n",
    "        self.input_proj = LinearProjection(input_dim, dim, init_weights=init_weights, optional=True)\n",
    "\n",
    "        # blocks\n",
    "        if cond_dim is None:\n",
    "            block_ctor = VitBlock\n",
    "        else:\n",
    "            block_ctor = partial(DitBlock, cond_dim=cond_dim)\n",
    "        self.blocks = Sequential(\n",
    "            *[\n",
    "                block_ctor(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    init_weights=init_weights,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # prepare perceiver\n",
    "        self.pos_embed = ContinuousSincosEmbed(\n",
    "            dim=perc_dim,\n",
    "            ndim=ndim,\n",
    "        )\n",
    "        if cond_dim is None:\n",
    "            block_ctor = PerceiverBlock\n",
    "        else:\n",
    "            block_ctor = partial(DitPerceiverBlock, cond_dim=cond_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.query_proj = nn.Sequential(\n",
    "            LinearProjection(perc_dim, perc_dim, init_weights=init_weights),\n",
    "            nn.GELU(),\n",
    "            LinearProjection(perc_dim, perc_dim, init_weights=init_weights),\n",
    "        )\n",
    "        self.perc = block_ctor(dim=perc_dim, kv_dim=dim, num_heads=perc_num_heads, init_weights=init_weights)\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.LayerNorm(perc_dim, eps=1e-6),\n",
    "            LinearProjection(perc_dim, output_dim*num_images, init_weights=init_weights),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, output_pos, condition=None):\n",
    "        # check inputs\n",
    "        assert x.ndim == 3, \"expected shape (batch_size, num_latent_tokens, dim)\"\n",
    "        assert output_pos.ndim == 3, \"expected shape (batch_size, num_outputs, dim) num_outputs might be padded\"\n",
    "        if condition is not None:\n",
    "            assert condition.ndim == 2, \"expected shape (batch_size, cond_dim)\"\n",
    "\n",
    "        # pass condition to DiT blocks\n",
    "        cond_kwargs = {}\n",
    "        if condition is not None:\n",
    "            cond_kwargs[\"cond\"] = condition\n",
    "\n",
    "        # input projection\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # apply blocks\n",
    "        x = self.blocks(x, **cond_kwargs)\n",
    "\n",
    "        # create query\n",
    "        query = self.pos_embed(output_pos)\n",
    "        query = self.query_proj(query)\n",
    "\n",
    "        x = self.perc(q=query, kv=x, **cond_kwargs)\n",
    "        x = self.pred(x)\n",
    "        if self.unbatch_mode == \"dense_to_sparse_unpadded\":\n",
    "            # dense to sparse where no padding needs to be considered\n",
    "            x = einops.rearrange(\n",
    "                x,\n",
    "                \"batch_size seqlen dim -> (batch_size seqlen) dim\",\n",
    "            )\n",
    "        elif self.unbatch_mode == \"image\":\n",
    "            # rearrange to square image\n",
    "            height, width = 96, 192\n",
    "            x = einops.rearrange(\n",
    "                x,\n",
    "                \"batch_size (height width) dim -> batch_size dim height width\",\n",
    "                height=int(height),\n",
    "                width=int(width),\n",
    "            )\n",
    "            x = x.view(1, self.num_images, 3, height, width)\n",
    "            x = x.permute(0, 1, 3, 4, 2)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"invalid unbatch_mode '{self.unbatch_mode}'\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc67e3ff-64af-40f4-8d4c-d97235d22fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = latent.output_shape,\n",
    "    output_shape = (None, 3), # 3 Channels for RGB\n",
    "    static_ctx = {\"ndim\": 2}, # Images are 2D\n",
    "    output_mode=\"image\",  # New parameter to control output mode\n",
    "    num_images=2,\n",
    "    image_dims=(192,96),  # Optional: Tuple of (height, width) for non-square images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1eb0401-68cb-472a-991a-8ff75933ea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RansPerceiver(\n",
       "  (proj): LinearProjection(\n",
       "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (pos_embed): ContinuousSincosEmbed(dim=768)\n",
       "  (query_mlp): Mlp(\n",
       "    (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (perceiver): PerceiverBlock(\n",
       "    (norm1q): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (norm1kv): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): PerceiverAttention1d(\n",
       "      (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (drop_path1): DropPath(drop_prob=0.000)\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (drop_path2): DropPath(drop_prob=0.000)\n",
       "  )\n",
       "  (pred): LinearProjection(\n",
       "    (proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       "  (norm): Identity()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba274043-f187-4744-8db6-36c827921e82",
   "metadata": {},
   "source": [
    "### Suppose I have a Single Image (192,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1d16e06-27f5-45c1-94e7-94cc0312a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18432, 2])\n"
     ]
    }
   ],
   "source": [
    "# Generate positions for a single image: 192 x 96 = 18,432 tokens\n",
    "height, width = 192, 96\n",
    "base_grid = torch.stack(torch.meshgrid(\n",
    "    [torch.arange(height), torch.arange(width)], indexing=\"ij\"\n",
    "))  # [2, 192, 96]\n",
    "base_grid = einops.rearrange(base_grid, \"c h w -> (h w) c\")  # [18,432, 2]\n",
    "\n",
    "# shape => [1, 18432, 2]\n",
    "output_pos = base_grid.unsqueeze(0).float()\n",
    "\n",
    "print(output_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ce40fb6-4280-4b1d-99d3-3530670d3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated, query_pos=output_pos, unbatch_idx=batch_idx, unbatch_select=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4385aa06-1664-42fe-95cc-9c1fb24cdb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 96, 192, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80eb482c-ce59-432e-988a-fe5192514ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87345, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.ndata[\"y\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063df540-4317-48b9-919a-11e745219d35",
   "metadata": {},
   "source": [
    "### Alternative Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d03a4-fba2-4ba4-8db8-757274506391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A basic SIREN layer\n",
    "class SIRENLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, w0=30.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # Initialize following SIREN paper recommendations\n",
    "        # e.g., uniform_(-1 / in_features, 1 / in_features), etc.\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, ..., in_features]\n",
    "        return torch.sin(self.w0 * self.linear(x))\n",
    "\n",
    "class ModulatedSIREN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens=256,\n",
    "        dim=384,\n",
    "        hidden_dim=256,\n",
    "        depth=5, \n",
    "        out_frames=22,\n",
    "        out_channels=3,\n",
    "        height=192,\n",
    "        width=96\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim = dim\n",
    "        self.out_frames = out_frames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        # SIREN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SIRENLayer(3, hidden_dim))  # (x, y, frame) -> hidden\n",
    "        for _ in range(depth - 2):\n",
    "            self.layers.append(SIRENLayer(hidden_dim, hidden_dim))\n",
    "        self.final_layer = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "        # An MLP that predicts scale & shift for each hidden layer from the latent\n",
    "        # Suppose we have 2 parameters (scale, shift) for each hidden layer => 2*(depth-1)*hidden_dim is the total\n",
    "        # But simpler: produce a scale, shift per layer (not per channel). Up to you how fine-grained you want it.\n",
    "        self.modulation_mlp = nn.Sequential(\n",
    "            nn.Linear(num_tokens * dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2*(depth - 1))  # scale, shift for each of the hidden layers (except final)\n",
    "        )\n",
    "\n",
    "    def forward(self, latent):\n",
    "        \"\"\"\n",
    "        latent: [batch_size, num_tokens, dim]\n",
    "        Output: [batch_size, out_frames, height, width, out_channels]\n",
    "        \"\"\"\n",
    "        bsz = latent.size(0)\n",
    "\n",
    "        # Summarize the latent\n",
    "        # e.g., flatten -> MLP -> scale/shift parameters\n",
    "        latent_flat = latent.reshape(bsz, -1)  # [batch_size, num_tokens * dim]\n",
    "        mod_params = self.modulation_mlp(latent_flat)  # [batch_size, 2*(depth-1)]\n",
    "        # Reshape: [batch_size, depth-1, 2] => each layer has scale & shift\n",
    "        mod_params = mod_params.view(bsz, -1, 2)\n",
    "\n",
    "        # Prepare the coordinate grid\n",
    "        # We'll create a meshgrid of shape (height*width*out_frames) x 3 => (x, y, t)\n",
    "        # Normalize coordinates to [-1,1] or something similar.\n",
    "        xs = torch.linspace(-1, 1, steps=self.width, device=latent.device)\n",
    "        ys = torch.linspace(-1, 1, steps=self.height, device=latent.device)\n",
    "        ts = torch.linspace(-1, 1, steps=self.out_frames, device=latent.device)\n",
    "\n",
    "        # Create a 3D meshgrid\n",
    "        Y, X, T = torch.meshgrid(ys, xs, ts, indexing='ij')  # shape = [H, W, F]\n",
    "        coords = torch.stack([X, Y, T], dim=-1)  # [H, W, F, 3]\n",
    "        coords = coords.reshape(-1, 3)          # [H*W*F, 3]\n",
    "\n",
    "        # We'll process all coords in a batch\n",
    "        # Expand to [batch_size, H*W*F, 3]\n",
    "        coords = coords.unsqueeze(0).expand(bsz, -1, -1)\n",
    "\n",
    "        # Forward pass through SIREN\n",
    "        x = coords  # rename for clarity\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            x = layer(x)  # shape: [batch_size, HWF, hidden_dim] or [batch_size, HWF, out_features]\n",
    "            if layer_idx < len(self.layers) - 1:  # skip final layer\n",
    "                # Apply modulation (scale & shift) for this layer\n",
    "                scale = mod_params[:, layer_idx, 0].view(bsz, 1)  # [batch, 1]\n",
    "                shift = mod_params[:, layer_idx, 1].view(bsz, 1)  # [batch, 1]\n",
    "                x = x * (1 + scale) + shift\n",
    "\n",
    "        # Final layer to get RGB\n",
    "        x = self.final_layer(x)  # [batch_size, HWF, out_channels]\n",
    "\n",
    "        # Reshape to [batch_size, H, W, F, C]\n",
    "        x = x.view(bsz, self.out_frames, self.height, self.width, -1)\n",
    "\n",
    "        return x  # shape: [batch_size, H, W, F, 3]\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"A basic Transformer block with multi-head self-attention + MLP.\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(dim * mlp_ratio), dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        x2, _ = self.attn(x, x, x)\n",
    "        x = x + x2\n",
    "        x = self.norm1(x)\n",
    "        # MLP\n",
    "        x2 = self.mlp(x)\n",
    "        x = x + x2\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class VisionTransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens=256,\n",
    "        dim=384,\n",
    "        depth=4,\n",
    "        num_heads=8,\n",
    "        patch_size=16,\n",
    "        out_frames=22,\n",
    "        out_channels=3,\n",
    "        out_height=192,\n",
    "        out_width=96\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Positional embeddings for the decoder tokens (optional)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_tokens, dim))\n",
    "\n",
    "        # A stack of Transformer blocks\n",
    "        self.transformer = nn.ModuleList([\n",
    "            SimpleTransformerBlock(dim, num_heads=num_heads) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Suppose we decode into a patch grid for each frame, e.g. 12 x 6 patches = 72 patches,\n",
    "        # for each of the 22 frames, total 72 * 22 = 1584 tokens. That might be large.\n",
    "        # Simplify: let's decode fewer patches, then upscale with a small CNN.\n",
    "\n",
    "        # Create a learnable \"query\" token for each patch. \n",
    "        # Suppose we have total_patches = frames * patches_per_frame\n",
    "        self.patches_per_frame = (out_height // patch_size) * (out_width // patch_size)\n",
    "        self.total_output_tokens = out_frames * self.patches_per_frame\n",
    "\n",
    "        self.query_tokens = nn.Parameter(\n",
    "            torch.randn(1, self.total_output_tokens, dim)\n",
    "        )\n",
    "\n",
    "        # Final linear to go from dim -> patch_size*patch_size*out_channels\n",
    "        self.patch_projection = nn.Linear(dim, patch_size * patch_size * out_channels)\n",
    "\n",
    "        # A small upscaling CNN if we want to refine the patches into full resolution.\n",
    "        self.refine_cnn = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.out_frames = out_frames\n",
    "        self.patch_size = patch_size\n",
    "        self.out_channels = out_channels\n",
    "        self.out_height = out_height\n",
    "        self.out_width = out_width\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_tokens, dim]\n",
    "        Output: [batch_size, out_frames, out_height, out_width, out_channels]\n",
    "        \"\"\"\n",
    "        bsz = x.size(0)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding[:, : self.num_tokens, :]\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.transformer:\n",
    "            x = block(x)   # [batch_size, num_tokens, dim]\n",
    "\n",
    "        # Expand query tokens for each batch\n",
    "        query_tokens = self.query_tokens.repeat(bsz, 1, 1)  # [batch_size, total_output_tokens, dim]\n",
    "        \n",
    "        # Cross-attention: let the query tokens attend to the output x (which is the \"memory\")\n",
    "        # For simplicity, just do self-attention with the combined tokens. \n",
    "        # Or you could implement cross-attention specifically. Here is a simple approach:\n",
    "        combined = torch.cat([x, query_tokens], dim=1)  # shape: [B, 256 + total_output_tokens, dim]\n",
    "        for block in self.transformer:\n",
    "            combined = block(combined)\n",
    "\n",
    "        # The last part of combined are the query tokens\n",
    "        out_tokens = combined[:, self.num_tokens:, :]  # [B, total_output_tokens, dim]\n",
    "\n",
    "        # Convert tokens -> patches\n",
    "        patches = self.patch_projection(out_tokens)  # [B, total_output_tokens, patch_size^2 * out_channels]\n",
    "        patches = patches.view(bsz, \n",
    "                               self.total_output_tokens,\n",
    "                               self.out_channels,\n",
    "                               self.patch_size,\n",
    "                               self.patch_size)  # [B, T, C, pH, pW]\n",
    "\n",
    "        # Rearrange patches into (frames, height, width)\n",
    "        # We know total_output_tokens = out_frames * patches_per_frame\n",
    "        # patches_per_frame = (out_height // patch_size) * (out_width // patch_size)\n",
    "        patches = patches.view(bsz, \n",
    "                               self.out_frames,\n",
    "                               self.patches_per_frame,\n",
    "                               self.out_channels,\n",
    "                               self.patch_size,\n",
    "                               self.patch_size)  # [B, F, P, C, pH, pW]\n",
    "\n",
    "        # Now we need to tile these patches along height/width\n",
    "        # Suppose we laid them out row by row:\n",
    "        # e.g., out_height // patch_size rows, out_width // patch_size cols\n",
    "        row_count = self.out_height // self.patch_size\n",
    "        col_count = self.out_width // self.patch_size\n",
    "        \n",
    "        # Reshape to a grid\n",
    "        patches = patches.view(bsz, \n",
    "                               self.out_frames,\n",
    "                               row_count,\n",
    "                               col_count,\n",
    "                               self.out_channels,\n",
    "                               self.patch_size,\n",
    "                               self.patch_size)\n",
    "\n",
    "        # Combine row, col into a single H, W\n",
    "        patches = patches.permute(0, 1, 4, 2, 5, 3, 6)  # [B, F, C, row_count, pH, col_count, pW]\n",
    "        patches = patches.reshape(\n",
    "            bsz, \n",
    "            self.out_frames,\n",
    "            self.out_channels,\n",
    "            row_count * self.patch_size,\n",
    "            col_count * self.patch_size\n",
    "        )  # -> [B, F, C, out_height, out_width]\n",
    "\n",
    "        # Optional refine\n",
    "        # Flatten frames into batch dimension, run refine, then reshape back\n",
    "        patches = patches.view(bsz * self.out_frames, self.out_channels, self.out_height, self.out_width)\n",
    "        patches = self.refine_cnn(patches)\n",
    "        patches = patches.view(bsz, self.out_frames, self.out_channels, self.out_height, self.out_width)\n",
    "\n",
    "        # Permute to [B, F, H, W, C]\n",
    "        patches = patches.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        return patches\n",
    "\n",
    "class SimpleCNNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens=256,\n",
    "        dim=384,\n",
    "        hidden_dim=1024,\n",
    "        out_frames=22,\n",
    "        out_channels=3,\n",
    "        out_height=192,\n",
    "        out_width=96\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim = dim\n",
    "        self.out_frames = out_frames\n",
    "        self.out_channels = out_channels\n",
    "        self.out_height = out_height\n",
    "        self.out_width = out_width\n",
    "        \n",
    "        # Example: Project from (num_tokens, dim) -> Flatten -> FC -> feature map\n",
    "        # Let's create a seed feature map of shape (512, 12, 6) as an example\n",
    "        self.initial_fc = nn.Linear(num_tokens * dim, 512 * 12 * 6)\n",
    "        \n",
    "        # Build upsampling decoder layers\n",
    "        # Each layer roughly doubles the height/width until we reach (192,96).\n",
    "        # We also have out_frames * out_channels = 66 for the final.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), # -> (256, 24, 12)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> (128, 48, 24)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # -> (64, 96, 48)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, out_frames * out_channels, \n",
    "                               kernel_size=4, stride=2, padding=1),          # -> (66, 192, 96)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_tokens, dim]\n",
    "        We want output: [batch_size, out_frames, out_height, out_width, out_channels]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(batch_size, -1)  # shape: [batch_size, num_tokens * dim]\n",
    "        \n",
    "        # Map to initial feature map\n",
    "        x = self.initial_fc(x)  # [batch_size, 512*12*6]\n",
    "        x = x.view(batch_size, 512, 12, 6)  # [batch_size, 512, 12, 6]\n",
    "        \n",
    "        # Decode\n",
    "        x = self.decoder(x)  # [batch_size, out_frames * out_channels, out_height, out_width]\n",
    "        \n",
    "        # Reshape: [batch_size, out_frames, out_channels, out_height, out_width]\n",
    "        x = x.view(batch_size, self.out_frames, self.out_channels, \n",
    "                   self.out_height, self.out_width)\n",
    "        \n",
    "        # Optionally reorder dims to [batch_size, out_frames, out_height, out_width, out_channels]\n",
    "        x = x.permute(0, 1, 3, 4, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9633b-7821-44a4-a007-f25275abe0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SimpleCNNDecoder(\n",
    "    num_tokens=1024,\n",
    "    dim=768,\n",
    "    hidden_dim=1024,\n",
    "    out_frames=22,\n",
    "    out_channels=3,\n",
    "    out_height=192,\n",
    "    out_width=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbe495-3aa7-48bc-94f6-025177de2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d2eb6-334d-42d7-b906-ef25eb7947f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "propagated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5dd90-42ba-45e2-a421-0685084502ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa1c64-0b36-48ad-9fa5-c7bb1ed802db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb8307-f38d-45b2-93f7-a098f3467a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = VisionTransformerDecoder(\n",
    "    num_tokens=1024,\n",
    "    dim=768,\n",
    "    depth=4,\n",
    "    num_heads=8,\n",
    "    patch_size=16,\n",
    "    out_frames=22,\n",
    "    out_channels=3,\n",
    "    out_height=192,\n",
    "    out_width=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9d4fe-0ddc-49ad-ab75-128fa0799b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01bd42-0954-4644-9242-1f640c19820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfb101-1dc8-40b5-b967-65000c95b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a34237-4910-481c-bc9b-82ad938ceb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ModulatedSIREN(\n",
    "        num_tokens=1024,\n",
    "        dim=768,\n",
    "        hidden_dim=1024,\n",
    "        depth=4, \n",
    "        out_frames=22,\n",
    "        out_channels=3,\n",
    "        height=96,\n",
    "        width=192\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db6022-ee4d-4601-8b94-45495311e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e21ca-6f29-4f4b-8d29-73fcdd3a806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288d602-a19c-4e07-89c1-b77b378c0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb9eb2-de0a-4323-9be6-fa637ba9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "propagated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869318ed-8ddd-4ff7-a959-44ffbcf19e3f",
   "metadata": {},
   "source": [
    "### Switch to 3D Query POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea0ae7-dde1-43a7-94bf-170fe6d9cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "def generate_query_positions(num_images, height, width, device=None):\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape [num_images * height * width, 3],\n",
    "    where each row is (image_idx, y, x).\n",
    "    Then you'll typically unsqueeze(0) to get [1, num_images*height*width, 3]\n",
    "    if you want a single batch dimension.\n",
    "    \"\"\"\n",
    "    # We'll collect a list of (image_idx, y, x) for each image\n",
    "    coords_list = []\n",
    "    for img_idx in range(num_images):\n",
    "        # 1) Create a meshgrid for a single image: (y,x) => shape [2, height, width]\n",
    "        base_grid = torch.stack(\n",
    "            torch.meshgrid([\n",
    "                torch.arange(height, device=device),\n",
    "                torch.arange(width, device=device)\n",
    "            ], indexing=\"ij\")\n",
    "        )  # shape [2, height, width]\n",
    "        # rearrange to [height*width, 2]\n",
    "        base_grid = einops.rearrange(base_grid, \"c h w -> (h w) c\")  \n",
    "        \n",
    "        # 2) Create an image_idx vector for these pixels\n",
    "        image_idx = torch.full((base_grid.shape[0], 1), float(img_idx), device=device)\n",
    "        \n",
    "        # 3) Concatenate: [height*width, 3] => (image_idx, y, x)\n",
    "        coords = torch.cat([image_idx, base_grid], dim=-1)\n",
    "        coords_list.append(coords)\n",
    "    \n",
    "    # Combine all images into one big array: [num_images * height*width, 3]\n",
    "    all_coords = torch.cat(coords_list, dim=0)\n",
    "    return all_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad9c72-5aa1-4cda-be13-04f196983286",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 2\n",
    "height, width = 192, 96\n",
    "\n",
    "# shape => [2 * 192 * 96, 3]\n",
    "output_pos = generate_query_positions(num_images, height, width, device=\"cpu\")\n",
    "\n",
    "# Typically, you'd add a batch dimension => [1, (2*192*96), 3]\n",
    "output_pos = output_pos.unsqueeze(0)  \n",
    "print(\"output_pos.shape =\", output_pos.shape)\n",
    "# e.g. [1, 36864, 3] if num_images=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d5db5-6ba4-4894-a38a-68056c326e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "from kappamodules.layers import ContinuousSincosEmbed, LinearProjection\n",
    "from kappamodules.transformer import PerceiverBlock, Mlp\n",
    "from torch_geometric.utils import unbatch\n",
    "from torch import nn\n",
    "from models.base.single_model_base import SingleModelBase\n",
    "import math\n",
    "\n",
    "class RansPerceiver(SingleModelBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_attn_heads,\n",
    "        init_weights=\"xavier_uniform\",\n",
    "        init_last_proj_zero=False,\n",
    "        use_last_norm=False,\n",
    "        output_mode=\"sparse\",  \n",
    "        num_images=1,  # Number of \"image-slots\" in the channel dimension\n",
    "        image_dims=None,  # (height, width) for image output\n",
    "        last_activation=None,  # <-- New parameter\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `last_activation` can be:\n",
    "            - None                (no activation, unbounded)\n",
    "            - \"sigmoid\"          (clamps output to [0,1])\n",
    "            - \"tanh_shift_scale\" (maps output to [0,1] via 0.5 * (tanh(x) + 1))\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.num_images = num_images  \n",
    "        self.use_last_norm = use_last_norm\n",
    "        self.output_mode = output_mode\n",
    "        self.image_dims = image_dims\n",
    "        self.last_activation = last_activation  # <-- Store last_activation\n",
    "\n",
    "        # Input projection\n",
    "        _, input_dim = self.input_shape\n",
    "        self.proj = LinearProjection(input_dim, dim, init_weights=init_weights)\n",
    "\n",
    "        # Query tokens (positional embedding + MLP)\n",
    "        self.pos_embed = ContinuousSincosEmbed(dim=dim, ndim=self.static_ctx[\"ndim\"]+1)\n",
    "        self.query_mlp = Mlp(in_dim=dim, hidden_dim=dim, init_weights=init_weights)\n",
    "\n",
    "        # Transformer block (latent to tokens)\n",
    "        self.perceiver = PerceiverBlock(\n",
    "            dim=dim,\n",
    "            num_heads=num_attn_heads,\n",
    "            init_last_proj_zero=init_last_proj_zero,\n",
    "            init_weights=init_weights,\n",
    "        )\n",
    "\n",
    "        # Figure out final channels (e.g., if output_dim=3 (RGB) then final is 3 * num_images)\n",
    "        _, output_dim = self.output_shape\n",
    "        # final_channels = output_dim * num_images\n",
    "        final_channels = output_dim * 1\n",
    "\n",
    "        # Final projection\n",
    "        self.pred = LinearProjection(dim, final_channels, init_weights=init_weights)\n",
    "\n",
    "        # Optional normalization\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6) if use_last_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x, query_pos, unbatch_idx, unbatch_select):\n",
    "        \"\"\"\n",
    "        x          : [batch_size, latent_seq_len, dim]\n",
    "        query_pos  : [batch_size, height*width, pos_dim]\n",
    "        \"\"\"\n",
    "        # 1) Project input\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # 2) Create query embeddings\n",
    "        query_pos_embed = self.pos_embed(query_pos)      # [batch_size, height*width, dim]\n",
    "        query = self.query_mlp(query_pos_embed)          # [batch_size, height*width, dim]\n",
    "\n",
    "        # 3) Perceiver decoding\n",
    "        x = self.perceiver(q=query, kv=x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pred(x)  \n",
    "        # Now shape: [batch_size, height*width, output_dim*num_images]\n",
    "\n",
    "        # 4) Reshape / rearrange based on self.output_mode\n",
    "        if self.output_mode == \"sparse\":\n",
    "            # E.g. for point-cloud style outputs\n",
    "            x = einops.rearrange(\n",
    "                x, \n",
    "                \"batch_size max_num_points (channels) -> (batch_size max_num_points) channels\", \n",
    "                channels=self.output_shape[1]  # e.g. 3\n",
    "            )\n",
    "            unbatched = unbatch(x, batch=unbatch_idx)\n",
    "            x = torch.concat([unbatched[i] for i in unbatch_select])\n",
    "\n",
    "        elif self.output_mode == \"dense_to_sparse_unpadded\":\n",
    "            x = einops.rearrange(x, \"b seqlen c -> (b seqlen) c\")\n",
    "\n",
    "        elif self.output_mode == \"image\":\n",
    "            # Reshape to [batch_size, num_images, output_dim, height, width]\n",
    "            if self.image_dims is not None:\n",
    "                height, width = self.image_dims\n",
    "\n",
    "                # We expect exactly height*width tokens\n",
    "                expected_tokens = height * width * num_images\n",
    "                assert x.size(1) == expected_tokens, (\n",
    "                    f\"Expected {expected_tokens} tokens (height*width), got {x.size(1)}\"\n",
    "                )\n",
    "\n",
    "                batch_size = x.size(0)\n",
    "                output_dim = self.output_shape[1]  # e.g., 3 (RGB)\n",
    "\n",
    "                print(x.shape)\n",
    "                \n",
    "                # Step-by-step reshape:\n",
    "                # Currently => [b, height*width, num_images*output_dim]\n",
    "                # 1) [b, hw, num_images, output_dim]\n",
    "                x = x.view(batch_size, height * width, self.num_images, output_dim)\n",
    "\n",
    "                # 2) [b, num_images, output_dim, hw]\n",
    "                x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "                # 3) [b, num_images, output_dim, height, width]\n",
    "                x = x.view(batch_size, self.num_images, output_dim, height, width)\n",
    "\n",
    "                # Optional final dimension reorder if you prefer [b, num_images, width, height, output_dim]:\n",
    "                x = x.permute(0, 1, 4, 3, 2)\n",
    "            else:\n",
    "                raise ValueError(\"image_dims must be provided for 'image' output mode.\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output mode: {self.output_mode}\")\n",
    "\n",
    "        # 5) Apply optional final activation\n",
    "        if self.last_activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        elif self.last_activation == \"tanh_shift_scale\":\n",
    "            # Map from [-1, +1] to [0, 1]\n",
    "            x = 0.5 * (torch.tanh(x) + 1.0)\n",
    "        elif self.last_activation is not None:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported value for last_activation='{self.last_activation}'. \"\n",
    "                \"Use None, 'sigmoid', or 'tanh_shift_scale'.\"\n",
    "            )\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61711b83-8b01-414b-be31-a97f49d1c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = latent.output_shape,\n",
    "    output_shape = (None, 3), # 3 Channels for RGB\n",
    "    static_ctx = {\"ndim\": 2}, # Images are 2D\n",
    "    output_mode=\"image\",  # New parameter to control output mode\n",
    "    num_images=2,\n",
    "    image_dims=(192,96),  # Optional: Tuple of (height, width) for non-square images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bf8b7-cc4f-4e2b-aeb1-b209b87ba5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated, query_pos=output_pos, unbatch_idx=batch_idx, unbatch_select=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b088f-482b-4fb8-a146-782514625a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfedb54-6908-4bf0-8fd8-85a9ed7170ae",
   "metadata": {},
   "source": [
    "### Try with the Decoder Perceiver Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92798915-c6e1-4c63-a9a7-907657ddd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "from kappamodules.layers import ContinuousSincosEmbed, LinearProjection, Sequential\n",
    "from kappamodules.transformer import PerceiverBlock, DitPerceiverBlock, DitBlock\n",
    "from kappamodules.vit import VitBlock\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class DecoderPerceiver(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            ndim,\n",
    "            dim,\n",
    "            depth,\n",
    "            num_heads,\n",
    "            unbatch_mode=\"dense_to_sparse_unpadded\",\n",
    "            perc_dim=None,\n",
    "            perc_num_heads=None,\n",
    "            cond_dim=None,\n",
    "            init_weights=\"truncnormal002\",\n",
    "            num_images=1, \n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        perc_dim = perc_dim or dim\n",
    "        perc_num_heads = perc_num_heads or num_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.ndim = ndim\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.perc_dim = perc_dim\n",
    "        self.perc_num_heads = perc_num_heads\n",
    "        self.cond_dim = cond_dim\n",
    "        self.init_weights = init_weights\n",
    "        self.unbatch_mode = unbatch_mode\n",
    "        self.num_images = num_images\n",
    "\n",
    "        # input projection\n",
    "        self.input_proj = LinearProjection(input_dim, dim, init_weights=init_weights, optional=True)\n",
    "\n",
    "        # blocks\n",
    "        if cond_dim is None:\n",
    "            block_ctor = VitBlock\n",
    "        else:\n",
    "            block_ctor = partial(DitBlock, cond_dim=cond_dim)\n",
    "        self.blocks = Sequential(\n",
    "            *[\n",
    "                block_ctor(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    init_weights=init_weights,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # prepare perceiver\n",
    "        self.pos_embed = ContinuousSincosEmbed(\n",
    "            dim=perc_dim,\n",
    "            ndim=ndim,\n",
    "        )\n",
    "        if cond_dim is None:\n",
    "            block_ctor = PerceiverBlock\n",
    "        else:\n",
    "            block_ctor = partial(DitPerceiverBlock, cond_dim=cond_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.query_proj = nn.Sequential(\n",
    "            LinearProjection(perc_dim, perc_dim, init_weights=init_weights),\n",
    "            nn.GELU(),\n",
    "            LinearProjection(perc_dim, perc_dim, init_weights=init_weights),\n",
    "        )\n",
    "        self.perc = block_ctor(dim=perc_dim, kv_dim=dim, num_heads=perc_num_heads, init_weights=init_weights)\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.LayerNorm(perc_dim, eps=1e-6),\n",
    "            LinearProjection(perc_dim, output_dim*num_images, init_weights=init_weights),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, output_pos, condition=None):\n",
    "        # check inputs\n",
    "        assert x.ndim == 3, \"expected shape (batch_size, num_latent_tokens, dim)\"\n",
    "        assert output_pos.ndim == 3, \"expected shape (batch_size, num_outputs, dim) num_outputs might be padded\"\n",
    "        if condition is not None:\n",
    "            assert condition.ndim == 2, \"expected shape (batch_size, cond_dim)\"\n",
    "\n",
    "        # pass condition to DiT blocks\n",
    "        cond_kwargs = {}\n",
    "        if condition is not None:\n",
    "            cond_kwargs[\"cond\"] = condition\n",
    "\n",
    "        # input projection\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # apply blocks\n",
    "        x = self.blocks(x, **cond_kwargs)\n",
    "\n",
    "        # create query\n",
    "        query = self.pos_embed(output_pos)\n",
    "        query = self.query_proj(query)\n",
    "\n",
    "        x = self.perc(q=query, kv=x, **cond_kwargs)\n",
    "        x = self.pred(x)\n",
    "        if self.unbatch_mode == \"dense_to_sparse_unpadded\":\n",
    "            # dense to sparse where no padding needs to be considered\n",
    "            x = einops.rearrange(\n",
    "                x,\n",
    "                \"batch_size seqlen dim -> (batch_size seqlen) dim\",\n",
    "            )\n",
    "        elif self.unbatch_mode == \"image\":\n",
    "            # rearrange to square image\n",
    "            height, width = 96, 192\n",
    "            x = einops.rearrange(\n",
    "                x,\n",
    "                \"batch_size (height width) dim -> batch_size dim height width\",\n",
    "                height=int(height),\n",
    "                width=int(width),\n",
    "            )\n",
    "            x = x.view(1, self.num_images, 3, height, width)\n",
    "            x = x.permute(0, 1, 3, 4, 2)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"invalid unbatch_mode '{self.unbatch_mode}'\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2628d-68b0-406e-91b7-359aeee7388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=DecoderPerceiver(\n",
    "        # tell the decoder the dimension of the input (dim of approximator)\n",
    "        input_dim=768,\n",
    "        # 3 channels for RGB\n",
    "        output_dim=3,\n",
    "        # images have 2D coordinates\n",
    "        ndim=2,\n",
    "        # as in ViT-T\n",
    "        dim= 768,\n",
    "        num_heads=12,\n",
    "        # ViT-T has 12 blocks -> parameters are split evenly among encoder/approximator/decoder\n",
    "        depth=4,\n",
    "        # reshape to image after decoding\n",
    "        unbatch_mode=\"image\",\n",
    "        num_images=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264f63c-0763-4e08-a4d7-a06e67347899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4361df9-2a71-4db5-8b02-7edd144f511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate positions for a single image: 192 x 96 = 18,432 tokens\n",
    "height, width = 192, 96\n",
    "base_grid = torch.stack(torch.meshgrid(\n",
    "    [torch.arange(height), torch.arange(width)], indexing=\"ij\"\n",
    "))  # [2, 192, 96]\n",
    "base_grid = einops.rearrange(base_grid, \"c h w -> (h w) c\")  # [18,432, 2]\n",
    "\n",
    "# shape => [1, 18432, 2]\n",
    "output_pos = base_grid.unsqueeze(0).float()\n",
    "\n",
    "print(output_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb09f6-7604-47ca-8618-4db9fc2350c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decoder(propagated, output_pos=output_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42452756-24d1-411d-aed4-b1a35623e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9e458-1176-452e-841b-a4db342f3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 2, 96, 192, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upt_venv",
   "language": "python",
   "name": "upt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
