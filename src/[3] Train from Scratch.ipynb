{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5bce77-7321-4f35-b6ea-5a55c5ba8567",
   "metadata": {},
   "source": [
    "# Train from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f431d79-d427-4945-8e4a-c94f7d4b3081",
   "metadata": {},
   "source": [
    "### Load Stage HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c4fbc9-ed6d-4eb4-8baa-5dd1bda77e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "def resolve_references(data, context):\n",
    "    \"\"\"\n",
    "    Recursively resolve references in the YAML data using the context dictionary.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The YAML data.\n",
    "        context (dict): The context dictionary with variable definitions.\n",
    "\n",
    "    Returns:\n",
    "        dict: The YAML data with resolved references and preserved types.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: resolve_references(v, context) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [resolve_references(item, context) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Find all placeholders in the format ${...}\n",
    "        matches = re.findall(r'\\$\\{([^}]+)\\}', data)\n",
    "        for match in matches:\n",
    "            # Replace the placeholder with the corresponding value from the context\n",
    "            keys = match.split('.')\n",
    "            value = context\n",
    "            for key in keys:\n",
    "                value = value.get(key)\n",
    "                if value is None:\n",
    "                    break\n",
    "            if value is not None:\n",
    "                # Attempt to cast the interpolated value to the original type if needed\n",
    "                if isinstance(value, int):\n",
    "                    return int(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                elif isinstance(value, float):\n",
    "                    return float(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                elif isinstance(value, bool):\n",
    "                    return bool(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                data = data.replace(f\"${{{match}}}\", str(value))\n",
    "        return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def load_yaml_with_interpolation(file_path):\n",
    "    \"\"\"\n",
    "    Load a YAML file with variable interpolation into a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The YAML data with interpolated variables.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            # Resolve references in the YAML data\n",
    "            data = resolve_references(data, data)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error loading YAML file: {e}\")\n",
    "            return None\n",
    "    return data\n",
    "\n",
    "stage_hp = load_yaml_with_interpolation(\"yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f151a5f-0bc1-4d6e-a53a-f67bb6a2a3b5",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab63976-b360-489b-853b-ec704454e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import model_from_kwargs\n",
    "from models.base.composite_model_base import CompositeModelBase\n",
    "from utils.factory import create\n",
    "\n",
    "from models.encoders.rans_grid_convnext import RansGridConvnext\n",
    "from models.encoders.rans_perceiver import RansPerceiver as EncoderRansPerceiver\n",
    "from models.latent.transformer_model import TransformerModel\n",
    "from models.decoders.rans_perceiver import RansPerceiver as DecoderRansPerceiver\n",
    "\n",
    "class RansSimformerNognnSdfModel_CAEML(CompositeModelBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_encoder,\n",
    "            mesh_encoder,\n",
    "            latent,\n",
    "            decoder,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        common_kwargs = dict(\n",
    "            update_counter=self.update_counter,\n",
    "            path_provider=self.path_provider,\n",
    "            dynamic_ctx=self.dynamic_ctx,\n",
    "            static_ctx=self.static_ctx,\n",
    "            data_container=self.data_container,\n",
    "        )\n",
    "        # grid_encoder\n",
    "        self.grid_encoder = grid_encoder\n",
    "        # mesh_encoder\n",
    "        self.mesh_encoder = mesh_encoder\n",
    "        # latent\n",
    "        self.latent = latent\n",
    "        # decoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @property\n",
    "    def submodels(self):\n",
    "        return dict(\n",
    "            grid_encoder=self.grid_encoder,\n",
    "            mesh_encoder=self.mesh_encoder,\n",
    "            latent=self.latent,\n",
    "            decoder=self.decoder,\n",
    "        )\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def forward(self, mesh_pos, sdf, query_pos, batch_idx, unbatch_idx, unbatch_select):\n",
    "        outputs = {}\n",
    "\n",
    "        # encode data\n",
    "        grid_embed = self.grid_encoder(sdf)\n",
    "        mesh_embed = self.mesh_encoder(mesh_pos=mesh_pos, batch_idx=batch_idx)\n",
    "        embed = torch.concat([grid_embed, mesh_embed], dim=1)\n",
    "\n",
    "        # propagate\n",
    "        propagated = self.latent(embed)\n",
    "\n",
    "        # decode\n",
    "        x_hat = self.decoder(\n",
    "            propagated,\n",
    "            query_pos=query_pos,\n",
    "            unbatch_idx=unbatch_idx,\n",
    "            unbatch_select=unbatch_select,\n",
    "        )\n",
    "        outputs[\"x_hat\"] = x_hat\n",
    "\n",
    "        return outputs\n",
    "        \n",
    "grid_encoder = RansGridConvnext(\n",
    "    patch_size = 2,\n",
    "    kernel_size = 3,\n",
    "    depthwise = False,\n",
    "    global_response_norm = True,\n",
    "    depths = [ 2, 2, 2 ],\n",
    "    dims = [ 192, 384, 768 ],\n",
    "    upsample_size = 64,\n",
    "    upsample_mode = \"nearest\",\n",
    "    resolution = (32, 32, 32), # This is because they are separate from the Input Positions I guess.\n",
    "    concat_pos_to_sdf = True\n",
    ")\n",
    "\n",
    "mesh_encoder = EncoderRansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    num_output_tokens = 1024,\n",
    "    add_type_token = True,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = (None, 3)\n",
    ")\n",
    "\n",
    "latent = TransformerModel(\n",
    "    init_weights = \"truncnormal\",\n",
    "    drop_path_rate = 0.2,\n",
    "    drop_path_decay = False,\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    depth = 12,\n",
    "    input_shape = mesh_encoder.output_shape\n",
    ")\n",
    "\n",
    "decoder = DecoderRansPerceiver(\n",
    "    dim = 768,\n",
    "    num_attn_heads = 12,\n",
    "    init_weights = \"truncnormal\",\n",
    "    input_shape = latent.output_shape,\n",
    "    output_shape = (None, 1),\n",
    "    static_ctx = {\"ndim\":3} # Not Sure.\n",
    ")\n",
    "\n",
    "\n",
    "model = RansSimformerNognnSdfModel_CAEML(\n",
    "    grid_encoder,\n",
    "    mesh_encoder,\n",
    "    latent,\n",
    "    decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3a475-c2ae-401a-966f-e889cb9d4451",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d858a6bc-cfdf-4f06-96cb-f285a4738be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.shapenet_car import ShapenetCar\n",
    "from utils.data_container import DataContainer\n",
    "\n",
    "train_dataset = ShapenetCar(\n",
    "    split = \"train\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "test_dataset = ShapenetCar(\n",
    "    split = \"test\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "datasets = {\"train\": train_dataset, \"test\": test_dataset}\n",
    "data_container = DataContainer(\n",
    "    **datasets,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    config_provider=None,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8243ec1-426a-4ad9-b4a8-448586373f37",
   "metadata": {},
   "source": [
    "### Set Up Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da02e8b-fd96-40e0-9f6b-cbaa137f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.collators.rans_simformer_nognn_collator import RansSimformerNognnCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=RansSimformerNognnCollator(),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=RansSimformerNognnCollator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dfa77f-c95e-48b2-a5e3-7a63050041d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6448856-c849-41a7-be32-dca710f352ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "UseModeWrapperException",
     "evalue": "wrap kappadata.KDDataset into kappadata.ModeWrapper before calling __getitem__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUseModeWrapperException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/kappadata/datasets/kd_dataset.py:89\u001b[0m, in \u001b[0;36mKDDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UseModeWrapperException\n",
      "\u001b[0;31mUseModeWrapperException\u001b[0m: wrap kappadata.KDDataset into kappadata.ModeWrapper before calling __getitem__"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c8da89-1b79-49f4-b050-09f8a885ca5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UseModeWrapperException",
     "evalue": "wrap kappadata.KDDataset into kappadata.ModeWrapper before calling __getitem__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUseModeWrapperException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/kappadata/datasets/kd_dataset.py:89\u001b[0m, in \u001b[0;36mKDDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UseModeWrapperException\n",
      "\u001b[0;31mUseModeWrapperException\u001b[0m: wrap kappadata.KDDataset into kappadata.ModeWrapper before calling __getitem__"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64506-cb8b-41d9-ac77-c9b1d052b90f",
   "metadata": {},
   "source": [
    "### Set Up Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e549ff-6f31-48b6-81f1-97a119e47c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5.0e-4, weight_decay=0.05)\n",
    "total_updates = len(train_dataloader) * epochs\n",
    "warmup_updates = int(total_updates * 0.1)\n",
    "lrs = torch.concat(\n",
    "    [\n",
    "        # linear warmup\n",
    "        torch.linspace(0, optim.defaults[\"lr\"], warmup_updates),\n",
    "        # linear decay\n",
    "        torch.linspace(optim.defaults[\"lr\"], 0, total_updates - warmup_updates),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dd5c9-98a5-4cb5-867d-c43a68b7fe1f",
   "metadata": {},
   "source": [
    "### Set Up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c7a1b-0a67-48fb-a5f0-45a6abc44a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.rans_simformer_nognn_trainer import RansSimformerNognnTrainer\n",
    "\n",
    "trainer = RansSimformerNognnTrainer(\n",
    "    device = \"cuda\",\n",
    "    data_container = data_container,\n",
    "    loss_function = stage_hp[\"trainer\"][\"loss_function\"],\n",
    "    precision = 'bfloat16',\n",
    "    max_epochs = 100,\n",
    "    effective_batch_size = 1,\n",
    "    max_batch_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c66e3a-d821-4e8a-84fd-c3bf4f8e749b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f9ca6-d053-4c7c-85e4-967866b43f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "update = 0\n",
    "pbar = tqdm(total=total_updates)\n",
    "pbar.update(0)\n",
    "pbar.set_description(\n",
    "    f\"train_loss: ??????? \"\n",
    "    f\"rollout_loss: ???????\"\n",
    ")\n",
    "train_losses = []\n",
    "rollout_losses = []\n",
    "rollout_loss = 0.\n",
    "loss = None\n",
    "for _ in range(epochs):\n",
    "    # train for an epoch\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # schedule learning rate\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group[\"lr\"] = lrs[update]\n",
    "\n",
    "        # forward pass\n",
    "        y_hat = model(\n",
    "            input_feat=batch[\"input_feat\"].to(device),\n",
    "            input_pos=batch[\"input_pos\"].to(device),\n",
    "            supernode_idxs=batch[\"supernode_idxs\"].to(device),\n",
    "            batch_idx=batch[\"batch_idx\"].to(device),\n",
    "            output_pos=batch[\"output_pos\"].to(device),\n",
    "            timestep=batch[\"timestep\"].to(device),\n",
    "        )\n",
    "        y = batch[\"output_feat\"].to(device)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update step\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # status update\n",
    "        update += 1\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.6f} \"\n",
    "            f\"rollout_loss: {rollout_loss:.6f}\"\n",
    "        )\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    for test_batch in rollout_dataloader:\n",
    "        with torch.no_grad():\n",
    "            rollout_preds = model.rollout(\n",
    "                input_feat=test_batch[\"input_feat\"].to(device),\n",
    "                input_pos=test_batch[\"input_pos\"].to(device),\n",
    "                supernode_idxs=test_batch[\"supernode_idxs\"].to(device),\n",
    "                batch_idx=test_batch[\"batch_idx\"].to(device),\n",
    "            )\n",
    "            assert len(test_batch[\"output_feat\"]) == 1, \"batch_size for rollout should be 1\"\n",
    "            output_feat = test_batch[\"output_feat\"][0]\n",
    "            num_rollout_timesteps = len(output_feat)\n",
    "            rollout_loss = 0.\n",
    "            for i in range(num_rollout_timesteps):\n",
    "                pred = rollout_preds[i]\n",
    "                target = output_feat[i]\n",
    "                rollout_loss += F.mse_loss(pred, target.to(device))\n",
    "            rollout_loss /= num_rollout_timesteps\n",
    "        rollout_losses.append(rollout_loss)\n",
    "        pbar.set_description(\n",
    "            f\"train_loss: {loss.item():.6f} \"\n",
    "            f\"rollout_loss: {rollout_loss:.6f}\"\n",
    "        )\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59512094-8539-4ffa-94fb-a64e3533e0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upt_venv",
   "language": "python",
   "name": "upt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
