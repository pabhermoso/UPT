{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1537d984-0c63-43a8-9550-564ad7aab3c6",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae1ad2fb-47fd-4f8b-a404-e6e837081c6c",
   "metadata": {},
   "source": [
    "# init datasets\n",
    "logging.info(\"------------------\")\n",
    "logging.info(\"initializing datasets\")\n",
    "datasets = {}\n",
    "dataset_config_provider = DatasetConfigProvider(\n",
    "    global_dataset_paths=static_config.get_global_dataset_paths(),\n",
    "    local_dataset_path=static_config.get_local_dataset_path(),\n",
    "    data_source_modes=static_config.get_data_source_modes(),\n",
    ")\n",
    "if \"datasets\" not in stage_hp:\n",
    "    logging.info(f\"no datasets found -> initialize dummy dataset\")\n",
    "    datasets[\"train\"] = DummyDataset(\n",
    "        size=256,\n",
    "        x_shape=(2,),\n",
    "        n_classes=2,\n",
    "    )\n",
    "else:\n",
    "    for dataset_key, dataset_kwargs in stage_hp[\"datasets\"].items():\n",
    "        logging.info(f\"initializing {dataset_key}\")\n",
    "        datasets[dataset_key] = dataset_from_kwargs(\n",
    "            dataset_config_provider=dataset_config_provider,\n",
    "            path_provider=path_provider,\n",
    "            **dataset_kwargs,\n",
    "        )\n",
    "data_container_kwargs = {}\n",
    "if \"prefetch_factor\" in stage_hp:\n",
    "    data_container_kwargs[\"prefetch_factor\"] = stage_hp.pop(\"prefetch_factor\")\n",
    "if \"max_num_workers\" in stage_hp:\n",
    "    data_container_kwargs[\"max_num_workers\"] = stage_hp.pop(\"max_num_workers\")\n",
    "data_container = DataContainer(\n",
    "    **datasets,\n",
    "    num_workers=cli_args.num_workers,\n",
    "    pin_memory=cli_args.pin_memory,\n",
    "    config_provider=config_provider,\n",
    "    seed=get_random_int(),\n",
    "    **data_container_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a7f3ece-aa68-40e0-a5ec-7c064e27c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "def resolve_references(data, context):\n",
    "    \"\"\"\n",
    "    Recursively resolve references in the YAML data using the context dictionary.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The YAML data.\n",
    "        context (dict): The context dictionary with variable definitions.\n",
    "\n",
    "    Returns:\n",
    "        dict: The YAML data with resolved references and preserved types.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: resolve_references(v, context) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [resolve_references(item, context) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Find all placeholders in the format ${...}\n",
    "        matches = re.findall(r'\\$\\{([^}]+)\\}', data)\n",
    "        for match in matches:\n",
    "            # Replace the placeholder with the corresponding value from the context\n",
    "            keys = match.split('.')\n",
    "            value = context\n",
    "            for key in keys:\n",
    "                value = value.get(key)\n",
    "                if value is None:\n",
    "                    break\n",
    "            if value is not None:\n",
    "                # Attempt to cast the interpolated value to the original type if needed\n",
    "                if isinstance(value, int):\n",
    "                    return int(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                elif isinstance(value, float):\n",
    "                    return float(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                elif isinstance(value, bool):\n",
    "                    return bool(data.replace(f\"${{{match}}}\", str(value)))\n",
    "                data = data.replace(f\"${{{match}}}\", str(value))\n",
    "        return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def load_yaml_with_interpolation(file_path):\n",
    "    \"\"\"\n",
    "    Load a YAML file with variable interpolation into a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The YAML data with interpolated variables.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            # Resolve references in the YAML data\n",
    "            data = resolve_references(data, data)\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error loading YAML file: {e}\")\n",
    "            return None\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e7ad134-db79-4e49-8f99-d8a6ac581414",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_hp = load_yaml_with_interpolation(\"yamls/shapenetcar/upt/dim768_seq1024sdf512_cnext_lr5e4_sd02_reprcnn_grn_grid32.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9584d9d2-4694-4a16-93f3-0086eb6cacf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'kind': 'shapenet_car',\n",
       "  'split': 'train',\n",
       "  'grid_resolution': 32,\n",
       "  'concat_pos_to_sdf': True,\n",
       "  'collators': [{'kind': 'rans_simformer_nognn_collator'}]},\n",
       " 'test': {'kind': 'shapenet_car',\n",
       "  'split': 'test',\n",
       "  'grid_resolution': 32,\n",
       "  'concat_pos_to_sdf': True,\n",
       "  'collators': [{'kind': 'rans_simformer_nognn_collator'}]}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_hp[\"datasets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5feb0c37-1b44-4e13-9782-5731cce387ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.static_config import StaticConfig\n",
    "from datasets import dataset_from_kwargs\n",
    "from providers.dataset_config_provider import DatasetConfigProvider\n",
    "from providers.path_provider import PathProvider\n",
    "from wandb.util import generate_id\n",
    "from distributed.config import is_rank0, is_distributed, get_rank, log_distributed_config\n",
    "from utils.data_container import DataContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f314ae46-5475-408e-88cb-88d82bb65c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "initializing datasets\n",
      "initializing train\n",
      "initializing test\n"
     ]
    }
   ],
   "source": [
    "# retrieve stage_id from hp (allows queueing up dependent stages by hardcoding stage_ids in the yamls) e.g.:\n",
    "# - pretrain MAE with stageid abcdefgh\n",
    "# - finetune MAE where the backbone is initialized with the backbone from stage_id abcdefgh\n",
    "stage_id = stage_hp.get(\"stage_id\", None)\n",
    "# generate stage_id and sync across devices\n",
    "if stage_id is None:\n",
    "    stage_id = generate_id()\n",
    "    if is_distributed():\n",
    "        object_list = [stage_id] if is_rank0() else [None]\n",
    "        broadcast_object_list(object_list)\n",
    "        stage_id = object_list[0]\n",
    "stage_name = stage_hp.get(\"stage_name\", \"default_stage\")\n",
    "\n",
    "static_config = StaticConfig(uri=\"static_config.yaml\", datasets_were_preloaded=False)\n",
    "\n",
    "path_provider = PathProvider(\n",
    "    output_path=static_config.output_path,\n",
    "    model_path=static_config.model_path,\n",
    "    stage_name=stage_name,\n",
    "    stage_id=stage_id,\n",
    "    temp_path=static_config.temp_path,\n",
    ")\n",
    "\n",
    "# init datasets\n",
    "print(\"------------------\")\n",
    "print(\"initializing datasets\")\n",
    "datasets = {}\n",
    "dataset_config_provider = DatasetConfigProvider(\n",
    "    global_dataset_paths=static_config.get_global_dataset_paths(),\n",
    "    local_dataset_path=static_config.get_local_dataset_path(),\n",
    "    data_source_modes=static_config.get_data_source_modes(),\n",
    ")\n",
    "\n",
    "for dataset_key, dataset_kwargs in stage_hp[\"datasets\"].items():\n",
    "    print(f\"initializing {dataset_key}\")\n",
    "    datasets[dataset_key] = dataset_from_kwargs(\n",
    "        dataset_config_provider=dataset_config_provider,\n",
    "        path_provider=None,\n",
    "        **dataset_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c518839-9492-4186-bea4-a0df205e9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container_kwargs = {}\n",
    "if \"prefetch_factor\" in stage_hp:\n",
    "    data_container_kwargs[\"prefetch_factor\"] = stage_hp.pop(\"prefetch_factor\")\n",
    "if \"max_num_workers\" in stage_hp:\n",
    "    data_container_kwargs[\"max_num_workers\"] = stage_hp.pop(\"max_num_workers\")\n",
    "data_container = DataContainer(\n",
    "    **datasets,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    config_provider=None,\n",
    "    seed=0,\n",
    "    **data_container_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a21483-a5f2-4686-910c-9e231fd53928",
   "metadata": {},
   "source": [
    "### Now try to Import it from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9cd0511f-9f71-4811-9866-1354ba9baa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.shapenet_car import ShapenetCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7265e036-09a7-47a9-bc41-6a21480eb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ShapenetCar(\n",
    "    split = \"train\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "test_dataset = ShapenetCar(\n",
    "    split = \"test\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576fdf1-42a0-4477-b9f9-cbd55dc1a656",
   "metadata": {},
   "source": [
    "### Try a Regular Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3aaccafc-dfd2-4a8f-9541-aa125df6dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_source (global): '/home/ubuntu/UPT/data/shapenet_car_processed'\n",
      "data_source (local): '/home/ubuntu/UPT/data/shapenet_car'\n",
      "data_source (global): '/home/ubuntu/UPT/data/shapenet_car_processed'\n",
      "data_source (local): '/home/ubuntu/UPT/data/shapenet_car'\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "import scipy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import meshio\n",
    "import numpy as np\n",
    "import torch\n",
    "from kappautils.param_checking import to_3tuple, to_2tuple\n",
    "from torch_geometric.nn.pool import radius, radius_graph\n",
    "\n",
    "from distributed.config import barrier, is_data_rank0\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from utils.param_checking import to_path\n",
    "\n",
    "class ShapenetCar(Dataset):\n",
    "    # generated with torch.randperm(889, generator=torch.Generator().manual_seed(0))[:189]\n",
    "    TEST_INDICES = {\n",
    "        550, 592, 229, 547, 62, 464, 798, 836, 5, 732, 876, 843, 367, 496,\n",
    "        142, 87, 88, 101, 303, 352, 517, 8, 462, 123, 348, 714, 384, 190,\n",
    "        505, 349, 174, 805, 156, 417, 764, 788, 645, 108, 829, 227, 555, 412,\n",
    "        854, 21, 55, 210, 188, 274, 646, 320, 4, 344, 525, 118, 385, 669,\n",
    "        113, 387, 222, 786, 515, 407, 14, 821, 239, 773, 474, 725, 620, 401,\n",
    "        546, 512, 837, 353, 537, 770, 41, 81, 664, 699, 373, 632, 411, 212,\n",
    "        678, 528, 120, 644, 500, 767, 790, 16, 316, 259, 134, 531, 479, 356,\n",
    "        641, 98, 294, 96, 318, 808, 663, 447, 445, 758, 656, 177, 734, 623,\n",
    "        216, 189, 133, 427, 745, 72, 257, 73, 341, 584, 346, 840, 182, 333,\n",
    "        218, 602, 99, 140, 809, 878, 658, 779, 65, 708, 84, 653, 542, 111,\n",
    "        129, 676, 163, 203, 250, 209, 11, 508, 671, 628, 112, 317, 114, 15,\n",
    "        723, 746, 765, 720, 828, 662, 665, 399, 162, 495, 135, 121, 181, 615,\n",
    "        518, 749, 155, 363, 195, 551, 650, 877, 116, 38, 338, 849, 334, 109,\n",
    "        580, 523, 631, 713, 607, 651, 168,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            split,\n",
    "            radius_graph_r=None,\n",
    "            radius_graph_max_num_neighbors=None,\n",
    "            num_input_points_ratio=None,\n",
    "            num_query_points_ratio=None,\n",
    "            grid_resolution=None,\n",
    "            num_supernodes=None,\n",
    "            standardize_query_pos=False,\n",
    "            concat_pos_to_sdf=False,\n",
    "            global_root=None,\n",
    "            local_root=None,\n",
    "            seed=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.split = split\n",
    "        self.radius_graph_r = radius_graph_r\n",
    "        self.radius_graph_max_num_neighbors = radius_graph_max_num_neighbors or int(1e10)\n",
    "        self.num_supernodes = num_supernodes\n",
    "        self.seed = seed\n",
    "        if num_input_points_ratio is None:\n",
    "            self.num_input_points_ratio = None\n",
    "        else:\n",
    "            self.num_input_points_ratio = to_2tuple(num_input_points_ratio)\n",
    "        self.num_query_points_ratio = num_query_points_ratio\n",
    "        if grid_resolution is not None:\n",
    "            self.grid_resolution = to_3tuple(grid_resolution)\n",
    "        else:\n",
    "            self.grid_resolution = None\n",
    "\n",
    "        # define spatial min/max of simulation (for normalizing to [0, 1] and then scaling to [0, 200] for pos_embed)\n",
    "        # min: [-1.7978, -0.7189, -4.2762]\n",
    "        # max: [1.8168, 4.3014, 5.8759]\n",
    "        self.domain_min = torch.tensor([-2.0, -1.0, -4.5])\n",
    "        self.domain_max = torch.tensor([2.0, 4.5, 6.0])\n",
    "        self.scale = 200\n",
    "        self.standardize_query_pos = standardize_query_pos\n",
    "        self.concat_pos_to_sdf = concat_pos_to_sdf\n",
    "\n",
    "        # mean/std for normalization (calculated on the 700 train samples)\n",
    "        # import torch\n",
    "        # from datasets.shapenet_car import ShapenetCar\n",
    "        # ds = ShapenetCar(global_root=\"/local00/bioinf/shapenet_car\", split=\"train\")\n",
    "        # targets = [ds.getitem_pressure(i) for i in range(len(ds))]\n",
    "        # targets = torch.stack(targets)\n",
    "        # targets.mean()\n",
    "        # targets.std()\n",
    "        self.mean = torch.tensor(-36.3099)\n",
    "        self.std = torch.tensor(48.5743)\n",
    "\n",
    "        # source_root\n",
    "        global_root, local_root = self._get_roots(global_root, local_root, \"shapenet_car\")\n",
    "        if local_root is None:\n",
    "            # load data from global_root\n",
    "            self.source_root = global_root / \"preprocessed\"\n",
    "            print(f\"data_source (global): '{self.source_root}'\")\n",
    "        else:\n",
    "            # load data from local_root\n",
    "            self.source_root = local_root / \"shapenet_car\"\n",
    "            if is_data_rank0():\n",
    "                # copy data from global to local\n",
    "                print(f\"data_source (global): '{global_root}'\")\n",
    "                print(f\"data_source (local): '{self.source_root}'\")\n",
    "                if not self.source_root.exists():\n",
    "                    print(\n",
    "                        f\"copying {(global_root / 'preprocessed').as_posix()} \"\n",
    "                        f\"to {(self.source_root / 'preprocessed').as_posix()}\"\n",
    "                    )\n",
    "                    shutil.copytree(global_root / \"preprocessed\", self.source_root / \"preprocessed\")\n",
    "            self.source_root = self.source_root / \"preprocessed\"\n",
    "            barrier()\n",
    "        assert self.source_root.exists(), f\"'{self.source_root.as_posix()}' doesn't exist\"\n",
    "        assert self.source_root.name == \"preprocessed\", f\"'{self.source_root.as_posix()}' is not preprocessed folder\"\n",
    "\n",
    "        # discover uris\n",
    "        self.uris = []\n",
    "        for i in range(9):\n",
    "            param_uri = self.source_root / f\"param{i}\"\n",
    "            for name in sorted(os.listdir(param_uri)):\n",
    "                sample_uri = param_uri / name\n",
    "                if sample_uri.is_dir():\n",
    "                    self.uris.append(sample_uri)\n",
    "        assert len(self.uris) == 889, f\"found {len(self.uris)} uris instead of 889\"\n",
    "        # split into train/test uris\n",
    "        if split == \"train\":\n",
    "            train_idxs = [i for i in range(len(self.uris)) if i not in self.TEST_INDICES]\n",
    "            self.uris = [self.uris[train_idx] for train_idx in train_idxs]\n",
    "            assert len(self.uris) == 700, f\"found {len(self.uris)} uris instead of 700\"\n",
    "        elif split == \"test\":\n",
    "            self.uris = [self.uris[test_idx] for test_idx in self.TEST_INDICES]\n",
    "            assert len(self.uris) == 189, f\"found {len(self.uris)} uris instead of 189\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uris)\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def getitem_pressure(self, idx, ctx=None):\n",
    "        p = torch.load(self.uris[idx] / \"pressure.th\")\n",
    "        p -= self.mean\n",
    "        p /= self.std\n",
    "        return p\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def getitem_grid_pos(self, idx=None, ctx=None):\n",
    "        if ctx is not None and \"grid_pos\" in ctx:\n",
    "            return ctx[\"grid_pos\"]\n",
    "        # generate positions for a regular grid (e.g. for GINO encoder)\n",
    "        assert self.grid_resolution is not None\n",
    "        x_linspace = torch.linspace(0, self.scale, self.grid_resolution[0])\n",
    "        y_linspace = torch.linspace(0, self.scale, self.grid_resolution[1])\n",
    "        z_linspace = torch.linspace(0, self.scale, self.grid_resolution[2])\n",
    "        # generate positions (grid_resolution[0] * grid_resolution[1], 2)\n",
    "        meshgrid = torch.meshgrid(x_linspace, y_linspace, z_linspace, indexing=\"ij\")\n",
    "        grid_pos = torch.stack(meshgrid).flatten(start_dim=1).T\n",
    "        #\n",
    "        if ctx is not None:\n",
    "            assert \"grid_pos\" not in ctx\n",
    "            ctx[\"grid_pos\"] = grid_pos\n",
    "        return grid_pos\n",
    "\n",
    "    def getitem_mesh_to_grid_edges(self, idx, ctx=None):\n",
    "        assert self.grid_resolution is not None\n",
    "        assert self.radius_graph_r is not None\n",
    "        mesh_pos = self.getitem_mesh_pos(idx, ctx=ctx)\n",
    "        grid_pos = self.getitem_grid_pos(idx, ctx=ctx)\n",
    "        # create graph between mesh and regular grid points\n",
    "        edges = radius(\n",
    "            x=mesh_pos,\n",
    "            y=grid_pos,\n",
    "            r=self.radius_graph_r,\n",
    "            max_num_neighbors=self.radius_graph_max_num_neighbors,\n",
    "        ).T\n",
    "        # edges is (num_points, 2)\n",
    "        return edges\n",
    "\n",
    "    def getitem_grid_to_query_edges(self, idx, ctx=None):\n",
    "        assert self.grid_resolution is not None\n",
    "        assert self.radius_graph_r is not None\n",
    "        query_pos = self.getitem_query_pos(idx, ctx=ctx)\n",
    "        grid_pos = self.getitem_grid_pos(idx, ctx=ctx)\n",
    "        # create graph between mesh and regular grid points\n",
    "        edges = radius(\n",
    "            x=grid_pos,\n",
    "            y=query_pos,\n",
    "            r=self.radius_graph_r,\n",
    "            max_num_neighbors=int(1e10),\n",
    "        ).T\n",
    "        # edges is (num_points, 2)\n",
    "        return edges\n",
    "\n",
    "    def getitem_mesh_pos(self, idx, ctx=None):\n",
    "        if ctx is not None and \"mesh_pos\" in ctx:\n",
    "            return ctx[\"mesh_pos\"]\n",
    "        mesh_pos = self.getitem_all_pos(idx, ctx=ctx)\n",
    "        # sample mesh points\n",
    "        if self.num_input_points_ratio is not None:\n",
    "            if self.split == \"test\":\n",
    "                assert self.seed is not None\n",
    "            if self.seed is not None:\n",
    "                # deterministically downsample for evaluation\n",
    "                generator = torch.Generator().manual_seed(self.seed + int(idx))\n",
    "            else:\n",
    "                generator = None\n",
    "            # get number of samples\n",
    "            if self.num_input_points_ratio[0] == self.num_input_points_ratio[1]:\n",
    "                # fixed num_input_points_ratio\n",
    "                end = int(len(mesh_pos) * self.num_input_points_ratio[0])\n",
    "            else:\n",
    "                # variable num_input_points_ratio\n",
    "                lb, ub = self.num_input_points_ratio\n",
    "                num_input_points_ratio = torch.rand(size=(1,), generator=generator).item() * (ub - lb) + lb\n",
    "                end = int(len(mesh_pos) * num_input_points_ratio)\n",
    "            # uniform sampling\n",
    "            perm = torch.randperm(len(mesh_pos), generator=generator)[:end]\n",
    "            mesh_pos = mesh_pos[perm]\n",
    "        if ctx is not None:\n",
    "            ctx[\"mesh_pos\"] = mesh_pos\n",
    "        return mesh_pos\n",
    "\n",
    "    def getitem_all_pos(self, idx, ctx=None):\n",
    "        if ctx is not None and \"all_pos\" in ctx:\n",
    "            return ctx[\"all_pos\"]\n",
    "        all_pos = torch.load(self.uris[idx] / \"mesh_points.th\")\n",
    "        # rescale for sincos positional embedding\n",
    "        all_pos.sub_(self.domain_min).div_(self.domain_max - self.domain_min).mul_(self.scale)\n",
    "        assert torch.all(0 < all_pos)\n",
    "        assert torch.all(all_pos < self.scale)\n",
    "        if ctx is not None:\n",
    "            ctx[\"all_pos\"] = all_pos\n",
    "        return all_pos\n",
    "\n",
    "    def getitem_query_pos(self, idx, ctx=None):\n",
    "        if ctx is not None and \"query_pos\" in ctx:\n",
    "            return ctx[\"query_pos\"]\n",
    "        query_pos = self.getitem_all_pos(idx, ctx=ctx)\n",
    "        # sample query points\n",
    "        if self.num_query_points_ratio is not None:\n",
    "            if self.split == \"test\":\n",
    "                assert self.seed is not None\n",
    "            if self.seed is not None:\n",
    "                # deterministically downsample for evaluation\n",
    "                generator = torch.Generator().manual_seed(self.seed + int(idx))\n",
    "            else:\n",
    "                generator = None\n",
    "            # get number of samples\n",
    "            end = int(len(query_pos) * self.num_query_points_ratio)\n",
    "            # uniform sampling\n",
    "            perm = torch.randperm(len(query_pos), generator=generator)[:end]\n",
    "            query_pos = query_pos[perm]\n",
    "        # shift query_pos to [-1, 1] (required for torch.nn.functional.grid_sample)\n",
    "        if self.standardize_query_pos:\n",
    "            query_pos = query_pos / (self.scale / 2) - 1\n",
    "        if ctx is not None:\n",
    "            ctx[\"query_pos\"] = query_pos\n",
    "        return query_pos\n",
    "\n",
    "    def _get_generator(self, idx):\n",
    "        if self.split == \"test\":\n",
    "            return torch.Generator().manual_seed(int(idx) + (self.seed or 0))\n",
    "        if self.seed is not None:\n",
    "            return torch.Generator().manual_seed(int(idx) + self.seed)\n",
    "        return None\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def getitem_mesh_edges(self, idx, ctx=None):\n",
    "        assert self.radius_graph_r is not None\n",
    "        # load mesh positions\n",
    "        mesh_pos = self.getitem_mesh_pos(idx, ctx=ctx)\n",
    "        if self.num_supernodes is None:\n",
    "            # create graph\n",
    "            edges = radius_graph(\n",
    "                x=mesh_pos,\n",
    "                r=self.radius_graph_r,\n",
    "                max_num_neighbors=self.radius_graph_max_num_neighbors,\n",
    "                loop=True,\n",
    "            )\n",
    "        else:\n",
    "            # select supernodes\n",
    "            generator = self._get_generator(idx)\n",
    "            perm = torch.randperm(len(mesh_pos), generator=generator)[:self.num_supernodes]\n",
    "            supernodes_pos = mesh_pos[perm]\n",
    "            # create edges: this can include self-loop or not depending on how many neighbors are found.\n",
    "            # if too many neighbors are found, neighbors are selected randomly which can discard the self-loop\n",
    "            edges = radius(\n",
    "                x=mesh_pos,\n",
    "                y=supernodes_pos,\n",
    "                r=self.radius_graph_r,\n",
    "                max_num_neighbors=self.radius_graph_max_num_neighbors,\n",
    "            )\n",
    "            # correct supernode index\n",
    "            edges[0] = perm[edges[0]]\n",
    "        return edges.T\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def getitem_sdf(self, idx, ctx=None):\n",
    "        assert self.grid_resolution is not None\n",
    "        assert all(self.grid_resolution[0] == grid_resolution for grid_resolution in self.grid_resolution[1:])\n",
    "        sdf = torch.load(self.uris[idx] / f\"sdf_res{self.grid_resolution[0]}.th\")\n",
    "        # check that sdf features were generated with correct positions by checking the distance to the nearest point\n",
    "        # from the domain minimum/maximum\n",
    "        # mesh_pos = torch.load(self.uris[idx] / \"mesh_points.th\")\n",
    "        # minpoint_dists = (self.domain_min[None, :] - mesh_pos).norm(p=2, dim=1)\n",
    "        # maxpoint_dists = (self.domain_max[None, :] - mesh_pos).norm(p=2, dim=1)\n",
    "        # assert torch.allclose(sdf[0, 0, 0], minpoint_dists.min()), f\"{sdf[0, 0, 0]} != {minpoint_dists.min()}\"\n",
    "        # assert torch.allclose(sdf[-1, -1, -1], maxpoint_dists.min()), f\"{sdf[-1, -1, -1]} != {maxpoint_dists.min()}\"\n",
    "        if self.concat_pos_to_sdf:\n",
    "            # add position to sdf (GINO uses this for interpolated FNO model)\n",
    "            x_linspace = torch.linspace(-1, 1, self.grid_resolution[0])\n",
    "            y_linspace = torch.linspace(-1, 1, self.grid_resolution[1])\n",
    "            z_linspace = torch.linspace(-1, 1, self.grid_resolution[2])\n",
    "            grid_pos = torch.meshgrid(x_linspace, y_linspace, z_linspace, indexing=\"ij\")\n",
    "            # stack features (models expect dim_last format)\n",
    "            sdf = torch.stack([sdf, *grid_pos], dim=-1)\n",
    "        else:\n",
    "            sdf = sdf.unsqueeze(-1)\n",
    "        return sdf\n",
    "\n",
    "\n",
    "    def getitem_interpolated(self, idx, ctx=None):\n",
    "        assert self.grid_resolution is not None\n",
    "        assert self.standardize_query_pos\n",
    "        mesh_pos = self.getitem_mesh_pos(idx, ctx=ctx)\n",
    "        # generate grid positions (these are different than getitem_gridpos because interpolate requires xy indexing)\n",
    "        # it should be the same if indexing=ij since the mapping and inverse mapping consider the change in indexing\n",
    "        # but for consistency with scipy.interpolate xy was chosen\n",
    "        x_linspace = torch.linspace(0, self.scale, self.grid_resolution[0])\n",
    "        y_linspace = torch.linspace(0, self.scale, self.grid_resolution[1])\n",
    "        z_linspace = torch.linspace(0, self.scale, self.grid_resolution[2])\n",
    "        grid_pos = torch.meshgrid(x_linspace, y_linspace, z_linspace, indexing=\"xy\")\n",
    "\n",
    "        grid = torch.from_numpy(\n",
    "            scipy.interpolate.griddata(\n",
    "                mesh_pos.unbind(1),\n",
    "                torch.ones_like(mesh_pos),\n",
    "                grid_pos,\n",
    "                method=\"linear\",\n",
    "                fill_value=0.,\n",
    "            ),\n",
    "        ).float()\n",
    "\n",
    "        # check for correctness of interpolation\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # import os\n",
    "        # os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "        # plt.scatter(mesh_pos[:, 0], mesh_pos[:, 1])\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "        # plt.imshow(grid.sum(dim=2).sum(dim=2), origin=\"lower\")\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "        # import torch.nn.functional as F\n",
    "        # grid = einops.rearrange(grid, \"h w d dim -> 1 dim h w d\")\n",
    "        # query_pos = self.getitem_query_pos(idx, ctx=ctx)\n",
    "        # query_pos = einops.rearrange(query_pos, \"num_points ndim -> 1 num_points 1 1 ndim\")\n",
    "        # mesh_values = F.grid_sample(input=grid, grid=query_pos, align_corners=False).squeeze(-1)\n",
    "        # plt.scatter(*query_pos.squeeze().unbind(1), c=mesh_values[0, 0, :, 0])\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "\n",
    "        return grid\n",
    "    \n",
    "    def _get_roots(self, global_root, local_root, dataset_identifier):\n",
    "        # automatically populate global_root/local_root if they are not defined explicitly\n",
    "        global_root = self._get_global_root(global_root, dataset_identifier)\n",
    "        if local_root is None:\n",
    "            if self.dataset_config_provider is not None:\n",
    "                source_mode = self.dataset_config_provider.get_data_source_mode(dataset_identifier)\n",
    "                # use local by default\n",
    "                if source_mode in [None, \"local\"]:\n",
    "                    local_root = self.dataset_config_provider.get_local_dataset_path()\n",
    "        else:\n",
    "            local_root = to_path(local_root)\n",
    "        return global_root, local_root\n",
    "\n",
    "    def _get_global_root(self, global_root, dataset_identifier):\n",
    "        if global_root is None:\n",
    "            global_root = self.dataset_config_provider.get_global_dataset_path(dataset_identifier)\n",
    "        else:\n",
    "            global_root = to_path(global_root)\n",
    "        return global_root\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_consistent_split(split, has_train=True, has_val=True, has_test=True):\n",
    "        if has_train and split in [\"train\", \"training\"]:\n",
    "            return \"train\"\n",
    "        if has_val and split in [\"val\", \"valid\", \"validation\"]:\n",
    "            return \"val\"\n",
    "        if has_test and split in [\"test\", \"testing\"]:\n",
    "            return \"test\"\n",
    "        raise NotImplementedError(\n",
    "            f\"invalid split '{split}' \"\n",
    "            f\"(has_train={has_train} has_val={has_val} has_test={has_test})\"\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        return type(self).__name__\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx\n",
    "\n",
    "train_dataset = ShapenetCar(\n",
    "    split = \"train\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "test_dataset = ShapenetCar(\n",
    "    split = \"test\",\n",
    "    grid_resolution = 32,\n",
    "    standardize_query_pos = False,\n",
    "    concat_pos_to_sdf = True,\n",
    "    global_root = '/home/ubuntu/UPT/data/shapenet_car_processed',\n",
    "    local_root = '/home/ubuntu/UPT/data',\n",
    "    seed=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e573a9f0-d9ce-455c-ba7d-32a6f51933c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.collators.rans_simformer_nognn_collator import RansSimformerNognnCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=RansSimformerNognnCollator(),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=RansSimformerNognnCollator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "138c8abe-11df-47b1-9708-a71267064199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43d55688-2652-4e24-b0d5-50c92a4e5d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "wrap KDSingleCollator with KDSingleCollatorWrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/UPT/src/datasets/collators/rans_simformer_nognn_collator.py:105\u001b[0m, in \u001b[0;36mRansSimformerNognnCollator.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrap KDSingleCollator with KDSingleCollatorWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: wrap KDSingleCollator with KDSingleCollatorWrapper"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1738236d-6aa7-49b4-8e5e-51ce484f9f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/UPT/upt_venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:61\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_co:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f7774-e979-4201-9ed5-031076a7badc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upt_venv",
   "language": "python",
   "name": "upt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
